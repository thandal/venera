{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"vcDc77Oq_21F"},"source":["References:\n","* https://iopscience.iop.org/article/10.3847/PSJ/ac4f43/pdf -- *Arecibo Radar Maps of Venus from 1988 to 2020 (overview paper)*\n","* https://pds-geosciences.wustl.edu/venus/arcb_nrao-v-rtls_gbt-3-delaydoppler-v1/vrm_90xx/document/venus_radar.pdf -- Level 1 doppler/dely data description\n","* https://pds-geosciences.wustl.edu/venus/urn-nasa-pds-venus_radar_level2/document/venus_radar_maps_user_guide.pdf -- Level 2 multi-look data description\n","* https://pubs.usgs.gov/of/1993/0516/report.pdf -- Venus Geologic Mappers' Handbook\n","* https://repository.si.edu/bitstream/handle/10088/3303/200737.pdf -- Details on the doppler/delay transformations\n","* https://echo.jpl.nasa.gov/asteroids/harmon.2002.long.code.pdf -- Details on the long-code doppler-delay method\n","\n","Tools:\n","* https://ssd.jpl.nasa.gov/horizons/app.html#/ -- JPL Horizons web app\n","* https://astroquery.readthedocs.io/en/latest/jplhorizons/jplhorizons.html -- JPL Horizons Python API\n","* https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/planets/de440_and_de441.pdf -- Ephemerides\n","* https://astropedia.astrogeology.usgs.gov/download/Docs/WGCCRE/WGCCRE2009reprint.pdf --  Report of the IAU Working Group on Cartographic Coordinates and Rotational Elements: 2009\n","* https://escholarship.org/content/qt2dn1012s/qt2dn1012s.pdf?t=pv3anr -- The mean rotation rate of Venus from 29â€¯years of Earth-based radar observations (also describes doppler/delay transformation)\n","\n","Maps:\n","* http://134.158.75.177/viewer/Apps/PlanetaryCesiumViewer/index.html?view=0%2C0%2C35255004%2C360%2C-90%2C0 -- with Venus lat/lon, etc.\n","* https://www.google.com/maps/space/venus/\n","* https://en.wikipedia.org/wiki/Mapping_of_Venus\n","* https://solarsystem.nasa.gov/resources/2342/venus-surface-3d-model/"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute Apparent Rotation Angle (Doppler Angle)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from astropy import units as au\n","from astropy import coordinates as ac\n","from astropy import constants as ak\n","from astropy import time as at\n","from poliastro.bodies import Venus\n","from poliastro.frames.fixed import VenusFixed\n","from poliastro.frames.equatorial import VenusICRS\n","from poliastro.spheroid_location import SpheroidLocation\n","import numpy as np\n","\n","# Set high quality ephemerides\n","# https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/planets/de440_and_de441.pdf appears to recommend de440s for our purposes:\n","# - de440s : 1849 to 2150 (32 MB)\n","# - de440 : 1550 to 2650 (114 MB)\n","# - de441 : -13200 to 17191(3.2 GB)\n","ac.solar_system_ephemeris.set(\"de440s\")  # JPL Horizons is using de441, which is in the same de44X family. \n","\n","def apparentRotationAngle_poliastro(obstime):\n","    # Observer point (Arecibo) to ICRS posvel\n","    o_coord = ac.EarthLocation.of_site('arecibo')\n","    o_gcrs = o_coord.get_gcrs(obstime)\n","    o_icrs = o_gcrs.transform_to(ac.ICRS())\n","\n","    # Venus Body Center\n","    vBC_fixed = VenusFixed(obstime=obstime,\n","                           x=0 * au.m, y=0 * au.m, z=0 * au.m,\n","                           v_x=0 * au.m / au.s, v_y=0 * au.m / au.s, v_z=0 * au.m / au.s,\n","                           representation_type='cartesian', differential_type='cartesian')\n","\n","    # Venus North Pole\n","    v = SpheroidLocation(0.0 * au.deg, 90.0 * au.deg, 0.0 * au.m, Venus)\n","    x, y, z = v.cartesian_cords\n","    vNP_fixed = VenusFixed(obstime=obstime,\n","                           x=x, y=y, z=z,\n","                           v_x=0 * au.m / au.s, v_y=0 * au.m / au.s, v_z=0 * au.m / au.s,\n","                           representation_type='cartesian', differential_type='cartesian')\n","\n","    VenusICRS(obstime=obstime) # HACK WORKAROUND\n","\n","    vBC_o_icrs = vBC_fixed.transform_to(o_icrs)\n","    vNP_o_icrs = vNP_fixed.transform_to(o_icrs)\n","\n","    # North Pole axis (in Arecibo frame)\n","    axis_NP = vNP_o_icrs.cartesian.without_differentials() - vBC_o_icrs.cartesian.without_differentials()\n","    axis_NP /= axis_NP.norm()\n","\n","    # Apparent Rotation axis (in Arecibo frame)\n","    axis_BC = vBC_o_icrs.cartesian.without_differentials() - o_icrs.cartesian.without_differentials()\n","    axis_BC /= axis_BC.norm()\n","    axis_AR = axis_BC.cross(vBC_o_icrs.velocity - o_icrs.velocity)\n","    axis_AR /= axis_AR.norm()\n","\n","    # Project the North Pole axis onto the plane of the sky\n","    sky_NP = axis_BC.cross(axis_NP.cross(axis_BC))\n","    sky_NP /= sky_NP.norm()\n","\n","    # Project the Apparent Rotation axis onto the plane of the sky\n","    sky_AR = axis_BC.cross(axis_AR.cross(axis_BC))\n","    sky_AR /= sky_AR.norm()\n","\n","    # Calculate the angle difference the two sky-projected axes\n","    sky_delta_angle = np.arccos(sky_NP.dot(sky_AR) / (sky_NP.norm() * sky_AR.norm()))\n","    if sky_delta_angle > 90 * au.deg: sky_delta_angle -= 180 * au.deg\n","    if sky_delta_angle < -90 * au.deg: sky_delta_angle += 180 * au.deg\n","    return sky_delta_angle"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from astroquery.jplhorizons import Horizons\n","\n","def apparentRotationAngle_horizons(obstime):\n","    # Venus points in Arecibo frame:\n","    # Venus Body Center\n","    obj = Horizons(id='299',  location='-1 @ 399', epochs=obstime.tdb.jd)\n","    vBC = obj.vectors()\n","    # Venus North Pole\n","    obj = Horizons(id='g: 0, 90, 0 @ 299',  location='-1 @ 399', epochs=obstime.tdb.jd)\n","    vNP = obj.vectors()\n","\n","    vBC_pos = ac.CartesianRepresentation(\n","        x=vBC['x'][0] * vBC['x'].unit,\n","        y=vBC['y'][0] * vBC['y'].unit,\n","        z=vBC['z'][0] * vBC['z'].unit)\n","    vBC_vel = ac.CartesianRepresentation(\n","        x=vBC['vx'][0] * vBC['vx'].unit,\n","        y=vBC['vy'][0] * vBC['vy'].unit,\n","        z=vBC['vz'][0] * vBC['vz'].unit)\n","\n","    vNP_pos = ac.CartesianRepresentation(\n","        x=vNP['x'][0] * vNP['x'].unit,\n","        y=vNP['y'][0] * vNP['y'].unit,\n","        z=vNP['z'][0] * vNP['z'].unit)\n","    vNP_vel = ac.CartesianRepresentation(\n","        x=vNP['vx'][0] * vNP['vx'].unit,\n","        y=vNP['vy'][0] * vNP['vy'].unit,\n","        z=vNP['vz'][0] * vNP['vz'].unit)\n","\n","    # North Pole axis\n","    axis_NP = vNP_pos - vBC_pos\n","    axis_NP /= axis_NP.norm()\n","\n","    # Apparent Rotation axis\n","    axis_PO = vBC_pos\n","    axis_PO /= axis_PO.norm()\n","    axis_AR = axis_PO.cross(vBC_vel)\n","    axis_AR /= axis_AR.norm()\n","\n","    # Project the North Pole axis onto the plane of the sky\n","    sky_NP = vBC_pos.cross(axis_NP.cross(vBC_pos))\n","    sky_NP /= sky_NP.norm()\n","\n","    # Project the Apparent Rotation axis onto the plane of the sky\n","    sky_AR = vBC_pos.cross(axis_AR.cross(vBC_pos))\n","    sky_AR /= sky_AR.norm()\n","\n","    # Calculate the angle difference the two sky-projected axes\n","    sky_delta_angle = np.arccos(sky_NP.dot(sky_AR) / (sky_NP.norm() * sky_AR.norm()))\n","    if sky_delta_angle > 90 * au.deg: sky_delta_angle -= 180 * au.deg\n","    if sky_delta_angle < -90 * au.deg: sky_delta_angle += 180 * au.deg\n","    return sky_delta_angle"]},{"cell_type":"markdown","metadata":{},"source":["## Arecibo Radar Data Processing"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# ! pip3 install --upgrade --quiet astroquery"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["_interactive = True"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["_interactive = False"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["ROOT_PREFIX = \"/mnt/c/Users/natha/Downloads/venus/arecibo_radar/pds-geosciences.wustl.edu/venus/arcb_nrao-v-rtls_gbt-3-delaydoppler-v1/vrm_90xx/\"\n","DATA_PREFIX = ROOT_PREFIX + \"data/\""]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669567231541,"user":{"displayName":"Nathaniel Fairfield","userId":"09088445613731179845"},"user_tz":300},"id":"tXBpaMJZbF03"},"outputs":[],"source":["if _interactive: ## Load real data.\n","    import numpy as np\n","\n","    # 1988:\n","    #filename = \"venus_ocp_19880604_163910.img\"  # First S (ever!)\n","    #filename = \"venus_ocp_19880620_144220.img\"\n","    #filename = \"venus_ocp_19880618_150330.img\"\n","    #filename = \"venus_ocp_19880617_151830.img\"\n","\n","    # 2001:\n","    #filename = \"venus_ocp_20010331_160048.img\"\n","    #filename = \"venus_ocp_20010331_184848.lbl\"\n","\n","    # 2012: used GBT, maybe avoid for now\n","\n","    # 2015:\n","    #filename = \"venus_ocp_20150810_162629.img\"  # First S\n","    #filename = \"venus_ocp_20150810_163623.img\"  # Next S\n","    #filename = \"venus_ocp_20150816_171104.img\"  # Last S\n","    #filename = \"venus_ocp_20150812_155242.img\"  # First N\n","    #filename = \"venus_scp_20150815_172030.img\"  # Last N\n","    filename = \"venus_ocp_20150812_155242.img\" # Bruce parameter comparison\n","    #filename = \"venus_ocp_20150815_170058.img\" # Bruce parameter comparison\n","\n","    # 2017:\n","\n","    # 2020:\n","    #filename = \"venus_scp_20200530_175421.img\"\n","    #filename = \"venus_ocp_20200530_175421.img\"\n","    #filename = \"venus_ocp_20200530_174429.img\"\n","    #filename = \"venus_ocp_20200530_173437.img\"\n","    #filename = \"venus_ocp_20200530_172445.img\"\n","\n","    # Vertical roll experiments\n","    #filename = 'venus_ocp_20010331_163648.img'  # Actually needs to be rolled *down* by a small amount\n","    #filename = 'venus_ocp_20120527_180159.img'  # Needs to be rolled up significantly\n","#    filename = 'venus_ocp_19880605_175820.img'  # Weak signal?\n","#    filename = 'venus_scp_19880604_163910.img'   # weak\n","#    filename = 'venus_scp_20200524_182906.img' #  weakish\n","\n","    # Fit experiments\n","    #filename = 'venus_ocp_19880604_163910.img'\n","\n","    # File format an 8191 * 8192 array of pixels, each pixel is a complex number represented by real, imaginary components stored as single-precision floats.\n","    # Early 1988 data files appear to be quite corrupt???\n","    img = np.memmap(DATA_PREFIX + filename, dtype='<F', shape=(8191, 8192), mode='r')  # 'F' is complex single-precision: a complex number type of 2 32-bit precision floating-point numbers.\n","    print(img.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def parseLbl(filename):\n","    lbl_filename = DATA_PREFIX + filename[:25] + \".lbl\"\n","    l = {}\n","    for line in open(lbl_filename).readlines():\n","        if 'START_TIME' in line:\n","            l['START_TIME'] = line.split('=')[-1].strip()\n","        if 'STOP_TIME' in line:\n","            l['STOP_TIME'] = line.split('=')[-1].strip()\n","        if 'GEO:BAUD' in line:\n","            l['GEO_BAUD'] = float(line.split(' ')[-2])\n","        if 'GEO:CENTROID_LOCATION' in line:\n","            l['GEO_CENTROID_LOCATION'] = int(line.split('=')[1].strip())\n","        if 'GEO:DELAY_OFFSET' in line:\n","            l['GEO_DELAY_OFFSET'] = int(line.split('=')[1].strip())\n","        if 'GEO:POINTING' in line:\n","            l['GEO_POINTING'] = line.split('\"')[-2]\n","        if 'GEO:MODE' in line:\n","            l['GEO_MODE'] = line.split('\"')[-2]\n","    return l\n","\n","if _interactive: \n","    lbl_dict = parseLbl(filename)\n","    print(f'{lbl_dict}')\n","\n","    # HACK: 1988 data seems to have flipped doppler?\n","    if lbl_dict['START_TIME'].startswith('1988'): img = np.fliplr(img)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["if 0: # Cache: Lookup and cache the JPL Horizons ephemerides data at observartion start and stop.\n","    import pickle\n","    import os\n","    from astropy import time as at\n","    from astroquery.jplhorizons import Horizons\n","\n","    for f in os.listdir(DATA_PREFIX):\n","        if not f.endswith('.img'): continue\n","        cache_filename = DATA_PREFIX + f[:-4] + '_horizons.pkl'\n","        if os.path.exists(cache_filename): continue\n","        print(f)\n","        lbl_dict = parseLbl(f)\n","        start_astrotime = at.Time(lbl_dict['START_TIME'])\n","        stop_astrotime = at.Time(lbl_dict['STOP_TIME'])\n","\n","        obj = Horizons(id='299',  # Venus\n","                       location='-1@399',  # Arecibo on Earth\n","                       epochs=[start_astrotime.jd, stop_astrotime.jd])\n","\n","        pickle.dump({\n","              'start_astrotime': start_astrotime,\n","              'stop_astrotime': stop_astrotime,\n","              'ephemerides': obj.ephemerides(),\n","              'vectors': obj.vectors(),\n","            },\n","            open(cache_filename, 'wb'))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["if 0: # debug: Compare North pole angle and SRP latitude\n","    import pickle\n","    import os\n","\n","    NP_ANG_DEG = []\n","    NP_DIS_ARCSEC = []\n","    SRP_LON_DEG = []\n","    SRP_LAT_DEG = []\n","    DOP_ANG_DEG = []\n","    for f in os.listdir(DATA_PREFIX):\n","        if not '1988' in f: continue\n","        #if not '2015' in f: continue\n","        #if not '2017' in f: continue\n","        #if not '2020' in f: continue\n","        if not f.endswith('horizons.pkl'): continue\n","        lbl_dict = parseLbl(f)\n","        if lbl_dict['GEO_POINTING'] != \"N\": continue\n","        SRP = pickle.load(open(DATA_PREFIX + f, 'rb'))\n","        eph = SRP['ephemerides']\n","        NP_ANG_DEG.append(eph['NPole_ang'][0])\n","        NP_DIS_ARCSEC.append(eph['NPole_dist'][0])\n","        SRP_LON_DEG.append(eph['PDObsLon'][0])\n","        SRP_LAT_DEG.append(eph['PDObsLat'][0])\n","        if 1: # debug: Compare apparent rotation angles\n","            sky_delta_angle_poliastro = apparentRotationAngle_poliastro(SRP['start_astrotime'])\n","            #sky_delta_angle_horizons = apparentRotationAngle_horizons(SRP['start_astrotime'])\n","            #print(f'{sky_delta_angle_poliastro.to(au.deg)=}')\n","            #print(f'{sky_delta_angle_horizons.to(au.deg)=}')\n","            DOP_ANG_DEG.append(sky_delta_angle_poliastro.to(au.deg).value)\n","            #DOP_ANG_DEG.append(sky_delta_angle_horizons.to(au.deg).value)\n","        #print(SRP['start_astrotime'], eph['PDObsLon'][0], eph['PDObsLat'][0])\n","    print(len(SRP_LON_DEG))\n","    print(np.mean(SRP_LON_DEG))\n","    print(np.mean(SRP_LAT_DEG))\n","\n","if 0:\n","    from matplotlib import pylab as plt\n","    #plt.plot(NP_DIS_ARCSEC)\n","    #plt.plot(NP_ANG_DEG)\n","    #NP_ANG = np.array(NP_ANG)\n","    #plt.plot(np.where(NP_ANG > 180, NP_ANG - 360, NP_ANG))\n","    #plt.plot(SRP_LAT)\n","    plt.hist(DOP_ANG_DEG)\n","    plt.title('Histogram of doppler angles (degrees)')\n","    plt.figure()\n","    plt.plot(SRP_LON_DEG, SRP_LAT_DEG, \"o\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["if _interactive: ## Load cached SRP data\n","    import pickle\n","    SRP = pickle.load(open(DATA_PREFIX + filename[:-4] + '_horizons.pkl', 'rb'))\n","    start_astrotime = SRP['start_astrotime']\n","    end_astrotime = SRP['start_astrotime']\n","    eph = SRP['ephemerides']\n","    SRP_lon_deg = eph['PDObsLon'][0]\n","    SRP_lat_deg = eph['PDObsLat'][0]\n","    print(start_astrotime)\n","    print(f'{SRP_lon_deg=}')\n","    print(f'{SRP_lat_deg=}')\n","    print(f\"{eph['NPole_ang'][0]=}\")\n","    if 1:\n","        sky_delta_angle_poliastro = apparentRotationAngle_poliastro(SRP['start_astrotime'])\n","        print(f'{sky_delta_angle_poliastro.to(au.deg).value=}')\n","        #sky_delta_angle_horizons = apparentRotationAngle_horizons(SRP['start_astrotime'])\n","        #print(f'{sky_delta_angle_horizons.to(au.deg).value=}')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def coarsePreprocessDopplerDelay(img):\n","    c2 = img.shape[1] // 2\n","    ## Convert complex valued image to magnitude image.\n","    img_a = np.abs(img)\n","    ## Normalize the image    \n","    img_a -= img_a.min()\n","    img_a /= img_a.max()\n","    ## Roll the frequency axis (left/right) by the default so that zero doppler is in the middle of the image, and the\n","    # planet looks like a planet.\n","    # NOTE: this is NOT a fine-tuned fit but should be correct to within ~100 columns.\n","    img_a = np.roll(img_a, c2)\n","    return img_a\n","\n","\n","def coarseTuneRollup(img_a):\n","    c2 = img_a.shape[1] // 2\n","    ## Roll the range axis (up/down) so that SRP onset starts at row 0\n","    # First, identify the biggest peak in the center-slice of the image.\n","    c = np.sum(img_a[:, 3500:-3500], axis=1)\n","    #plt.plot(c, 'r-')\n","    max_c = np.argmax(c)\n","    # Center the biggest peak to avoid wrapping issues.\n","    center_i = c2 - max_c\n","    c = np.roll(c, center_i)\n","    # Find the first big increase above the noise floor\n","    d = np.diff(c[c2 - 200:c2 + 1])\n","    #plt.plot(d, 'b-')\n","    d_pre_std = np.std(d[:c2 - 100])  # std from samples *before* the spike\n","    first_i = np.argwhere(d > d_pre_std * 2)[0][0]\n","    # Roll the image up/down to put first onset at row 0\n","    return -(max_c - 200 + first_i) - 1\n","\n","\n","def finePreprocessDopperDelay(img_a):\n","    # TODO: improve the echo power model -- AND map it properly to the angle of incidence!\n","    # TODO: apply a different rule for OCP? \n","    echo_power = 1 - np.cos(np.linspace(0, np.pi / 2, img_a.shape[0])) + 1e-1  # Add a litte to avoid zero.\n","    img_b = (img_a.T * echo_power).T\n","\n","    # TODO: \"noise normalize\"? (Mentioned in the papers)\n","    #noise_patch = img[:1000,:1000]\n","    #average_noise = np.mean(np.abs(noise_patch))\n","    #print('average_noise', average_noise)\n","    #img_b = img_a.copy()\n","    #img_b -= average_noise\n","\n","    # Flatten the 99th percentile\n","    percentile = 99\n","    percentile_thresh = np.percentile(img_b.ravel(), percentile)\n","    img_b = np.where(img_b > percentile_thresh, percentile_thresh, img_b)\n","\n","    #percentile = 5\n","    #percentile_thresh = np.percentile(img_b.ravel(), percentile)\n","    #img_b = np.where(img_b < percentile_thresh, percentile_thresh, img_b)\n","    #if filename: plt.imsave(f\"{ROOT_PREFIX}/PREPROCESS_TRIAGE/{filename[:-4]}_processed.png\", img_b, cmap=\"gray\")\n","    return img_b\n","\n","\n","if _interactive:\n","    img_a = coarsePreprocessDopplerDelay(img)\n","    best_rollup = coarseTuneRollup(img)\n","    img_a = np.roll(img_a, best_rollup, axis=0)\n","    img_b = finePreprocessDopperDelay(img_a)\n","\n","    if 1: # debug\n","        from matplotlib import pyplot as plt\n","        #plt.imsave(filename + \".png\", img_b)\n","        plt.figure(figsize=(16, 16))\n","        plt.axis('off')\n","        plt.imshow(img_b, cmap='gray')\n","\n","    if 0: # debug\n","        from matplotlib import pyplot as plt\n","        #plt.imsave(filename + \".png\", img_b)\n","        plt.figure(figsize=(16, 16))\n","        plt.axis('off')\n","        plt.imshow(img_b, cmap='gray')\n","    \n","    if 0:\n","        plt.figure(figsize=(16, 4))\n","        from matplotlib import pyplot as plt\n","        plt.hist(img_b.ravel(), bins=50)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def coarseTuneRoll(img_b, lbl_dict, filename=None): # Coarse-tune the centering by rolling to maximize left-right symmetry\n","    best_col_offset = 0\n","    best_col_offset_sum = 0\n","    # Baud roughly determines the frequency spreading, so we use different windows to avoid the wrap region.\n","    if lbl_dict[\"GEO_BAUD\"] == 4.2:\n","        img_h = img_b[2500:3000, :].copy()\n","    if lbl_dict[\"GEO_BAUD\"] == 4.0:\n","        img_h = img_b[3000:4000, :].copy()\n","    elif lbl_dict[\"GEO_BAUD\"] == 3.9:\n","        img_h = img_b[4000:5000, :].copy()\n","    elif lbl_dict[\"GEO_BAUD\"] == 3.8:  # For 2012\n","        img_h = img_b[3500:4500, :].copy()\n","    else:\n","        raise(\"Unknown GEO_BAUD\", lbl_dict[\"GEO_BAUD\"])\n","    for offset in range(-100, 101):  # HUGE range?\n","    #for offset in range(-50, 51):  # More sane range?\n","        img_tmp = np.roll(img_h, offset)\n","        #img_tmp = np.diff(img_tmp, axis=1, prepend=0)**2\n","        img_mirror = img_tmp[:, :1000] * np.fliplr(img_tmp[:, -1000:])\n","        #img_mirror = np.diff(img_tmp[:, :1000], axis=1)**2 * np.diff(np.fliplr(img_tmp[:, -1000:]), axis=1)**2  # Difference image\n","        #img_mirror = img_tmp * np.fliplr(img_tmp)\n","        #plt.imsave(filename + '_roll' + str(offset) + '.png', img_mirror)\n","        #plt.imshow(img_mirror, cmap='gray')\n","        total_sum = np.sum(img_mirror)\n","        #print(offset, total_sum)\n","        if total_sum > best_col_offset_sum:\n","            best_col_offset_sum = total_sum\n","            best_col_offset = offset\n","    if filename: # debug\n","        img_tmp = np.roll(img_h, best_col_offset)\n","        img_mirror = img_tmp[:, :1000] * np.fliplr(img_tmp[:, -1000:])\n","        plt.imsave(f\"{ROOT_PREFIX}/ROLL_TRIAGE/{filename[:-4]}_roll_{best_col_offset}.png\", img_mirror)\n","    return best_col_offset\n","    \n","if _interactive:\n","    best_col_offset = coarseTuneRoll(img_b, lbl_dict)\n","    print(f'{best_col_offset=}')\n","    img_b = np.roll(img_b, best_col_offset)\n","\n","    if 1: # debug\n","        from matplotlib import pyplot as plt\n","        #plt.imsave(filename + \".png\", img_b)\n","        plt.figure(figsize=(16, 16))\n","        plt.axis('off')\n","        plt.imshow(img_b + np.fliplr(img_b), cmap='gray')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def fitDopplerDelayCurve(img_b, lbl_dict, filename=None):\n","    c2 = img_b.shape[1] / 2\n","    _radius_km = 6051.8\n","    _row_dist_km = 299792.46 * lbl_dict['GEO_BAUD'] * 1e-6 / 2 # km  Note: row is *round-trip-time* (double distance)\n","    dlon = np.linspace(-np.pi/2, np.pi/2, 8000)\n","    row_dist = _radius_km * (1 - np.cos(dlon)) / _row_dist_km\n","    sin_dlon = np.sin(dlon)\n","\n","    # Grid search by looking for the best \"edge\"\n","    best_score = 0\n","    best_params = []\n","    for freq_offset in range(-3, 4):  # Small range because image has already been coarsely centered.\n","        for delay_offset in range(-3, 4):  # Small range because image has already been coarsely ranged.\n","            for freq_scale in np.linspace(0.8, 1.4, 301):\n","                cs = (c2 * freq_scale) * sin_dlon + (c2 + freq_offset)\n","                rs = row_dist + delay_offset\n","                rs = rs.astype('i')\n","                cs = cs.astype('i')\n","                cs2 = np.where(cs < c2, cs - 2, cs + 2)  # TODO: smarter kernel?\n","                row_start = 500 # NOTE: start at row_startto avoid bias due to extremely bright SRP return\n","                row_end = 6000  # NOTE: end at row_end to avoid noise due to frequency wrapping\n","                valid = (cs >= 0) & (cs < img_b.shape[1]) & (rs >= row_start) & (rs < row_end)\n","                valid2 = (cs2 >= 0) & (cs2 < img_b.shape[1]) & (rs >= row_start) & (rs < row_end)\n","                score = sum(img_b[rs[valid], cs[valid]]) - sum(img_b[rs[valid2], cs2[valid2]])\n","                if score > best_score:\n","                    best_score = score\n","                    best_params = [freq_offset, delay_offset, freq_scale]\n","                    #print(score, freq_offset, delay_offset, freq_scale)\n","    if filename:\n","        freq_offset, delay_offset, freq_scale = best_params\n","        cs = (c2 * freq_scale) * sin_dlon + (c2 + freq_offset)\n","        rs = row_dist + delay_offset\n","        rs = rs.astype('i')\n","        cs = cs.astype('i')\n","        cs2 = np.where(cs < c2, cs - 2, cs + 2)\n","        valid = (cs >= 0) & (cs < img_b.shape[1]) & (rs >= 0) & (rs < img_b.shape[0]) \n","        valid2 = (cs2 >= 0) & (cs2 < img_b.shape[1]) & (rs >= 0) & (rs < img_b.shape[0])   \n","        img_h = img_b.copy() # Scratch image\n","        img_h[rs[valid2], cs2[valid2]] = img_h.min() / 2\n","        img_h[rs[valid], cs[valid]] = img_h.max() * 2\n","        plt.imsave(f\"{ROOT_PREFIX}/FIT_TRIAGE/{filename[:-4]}_fit_{freq_offset}_{delay_offset}_{freq_scale}.png\", img_h, cmap='gray')\n","    return best_score, best_params\n","\n","if _interactive:\n","    best_fit_score, best_fit_parameters = fitDopplerDelayCurve(img_b, lbl_dict)\n","    freq_offset, delay_offset, freq_scale = best_fit_parameters\n","    print(f'Best fit score: {best_fit_score:.4f}')\n","    print(f'Best fit parameters: {freq_offset}, {delay_offset}, {freq_scale:.4f}')\n","    \n","    if 1:  # debug\n","        c2 = img_b.shape[1] / 2\n","        _radius_km = 6051.8\n","        _row_dist_km = 299792.46 * lbl_dict['GEO_BAUD'] * 1e-6 / 2 # km  Note: row is *round-trip-time* (double distance)\n","        dlon = np.linspace(-np.pi/2, np.pi/2, 8000)\n","        cs = (c2 * freq_scale) * np.sin(dlon) + (c2 + freq_offset)\n","        rs = _radius_km * (1 - np.cos(dlon)) / _row_dist_km + delay_offset\n","\n","        rs = rs.astype('i')\n","        cs = cs.astype('i')\n","        cs2 = np.where(cs < c2, cs - 2, cs + 2)\n","        valid = (cs >= 0) & (cs < img_b.shape[1]) & (rs >= 0) & (rs < img_b.shape[0]) \n","        valid2 = (cs2 >= 0) & (cs2 < img_b.shape[1]) & (rs >= 0) & (rs < img_b.shape[0])   \n","\n","        if 1:  # Debug image\n","            img_h = img_b.copy() # Scratch image\n","            #img_h[rs[valid], cs[valid]] = img_h.max()\n","            img_h[rs[valid2], cs2[valid2]] = img_h.min() / 2\n","            img_h[rs[valid], cs[valid]] = img_h.max() * 2\n","            plt.figure(figsize=(16, 16))\n","            plt.axis('off')\n","            plt.imshow(img_h, cmap='gray', interpolation='none')\n","            #plt.imsave(f\"{ROOT_PREFIX}/FIT_TRIAGE/{filename[:-4]}_fit.png\", img_h, cmap='gray')\n","        if 0: # Debug the curve fit.\n","            img_h = img_b.copy()\n","            #img_h = np.diff(img_c, axis=1, prepend=0)**2\n","            img_h[rs[valid2], cs2[valid2]] = img_h.min() / 2\n","            img_h[rs[valid], cs[valid]] = img_h.max() * 2\n","            plt.figure(figsize=(18, 12))\n","            plt.subplot(1,3,1)\n","            plt.axis('off')\n","            plt.imshow(img_h[4000:5000, :800], cmap='gray', interpolation='none')\n","            plt.subplot(1,3,2)\n","            plt.axis('off')\n","            plt.imshow(img_h[0:1000, 3000:-3000].T, cmap='gray')\n","            plt.subplot(1,3,3)\n","            plt.axis('off')\n","            plt.imshow(img_h[4000:5000, -800:], cmap='gray', interpolation='none')\n","            print('done')\n","        "]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def setLLV(G, Gc, lon, lat, v):\n","    # lon in [0, 2*pi)\n","    # lat in [-pi/2, pi/2]\n","    r = (lon / (2 * np.pi) * G.shape[0]).astype('i')\n","    c = ((lat + np.pi / 2) / np.pi * G.shape[1]).astype('i')\n","    G[r, c] = v\n","    Gc[r, c] = 1\n","\n","def addLLV(G, Gc, lon, lat, v):\n","    # lon in [0, 2*pi)\n","    # lat in [-pi/2, pi/2]\n","    r = (lon / (2 * np.pi) * G.shape[0]).astype('i')\n","    c = ((lat + np.pi / 2) / np.pi * G.shape[1]).astype('i')\n","    G[r, c] += v\n","    Gc[r, c] += 1"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["if 0:  # Draw debug lines of latitude and longitude\n","    for lon in range(360):\n","        setLL(G0, lon, 0, 1)\n","        setLL(G0, lon, 45, 0.5)\n","        setLL(G0, lon, -45, 0.2)\n","    \n","    for lat in range(-90, 90, 3):\n","        setLL(G0, 0, lat, 1)\n","        setLL(G0, 90, lat, 0.5)\n","        setLL(G0, 180, lat, 1)\n","        setLL(G0, 270, lat, 0.2)\n","    \n","    plt.figure(figsize=(12, 12))\n","    plt.axis('off')\n","    plt.imshow(G0.T, interpolation='none')\n","\n","if 0:\n","    #setLLV(G0, Gc, np.linspace(0, 359, 100), np.linspace(-90, 89, 100), 1)\n","    #setLLV(G, Gc, np.linspace(0, 359, 1000) / 180 * np.pi, np.linspace(66, 67, 1000) / 180 * np.pi, 1)\n","    setLLV(G, Gc, np.linspace(0, 359, 1000) / 180 * np.pi, np.linspace(66, 67, 1000) / 180 * np.pi, G.max())"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["if 0: # debug: derive the magic doppler angle scale (freq_scale factor)\n","    print(freq_scale)\n","    (-np.arccos(freq_scale/1.035) * au.radian).to(au.degree)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["## Project the doppler/delay image into lon/lat\n","# TODO: average pixel values?\n","\n","import astropy.constants as acon\n","import astropy.coordinates as ac\n","import astropy.units as au\n","\n","def dopplerDelayToSphericalProjection(img_b, G, Gc, lbl_dict, SRP, fit_parameters, fudge_deg=0):\n","    _radius_km = 6051.8\n","    _row_dist_km = 299792.46 * lbl_dict['GEO_BAUD'] * 1e-6 / 2 # km  Note: row is *round-trip-time* (double distance)\n","    freq_offset, delay_offset, freq_scale = fit_parameters\n","\n","    # Omit \"degraded data\" regions due to\n","    # - high N/S ambiguity near the edges (first 1 deg lat), and\n","    # - grazing angle (last 1 deg lat and lon)\n","    dlon = np.linspace(-85 / 180 * np.pi, 85 / 180 * np.pi, 8000)\n","    if lbl_dict['GEO_POINTING'] == 'N':\n","        dlat = np.linspace(5 / 180 * np.pi, 85 / 180 * np.pi, 4000)  # Northern hemisphere\n","    else:\n","        dlat = np.linspace(-85 / 180 * np.pi, -5 / 180 * np.pi, 4000)  # Southern hemisphere\n","    dlon_mesh, dlat_mesh = np.meshgrid(dlon, dlat)\n","\n","    # Omit degraded data region due to SRP (within 7 deg of the center)\n","    mesh_valid = np.sqrt(dlat_mesh**2 + dlon_mesh**2) > (7 / 180 * np.pi)\n","    dlon_mesh = dlon_mesh[mesh_valid]\n","    dlat_mesh = dlat_mesh[mesh_valid]\n","\n","    c2 = img_b.shape[1] / 2\n","    cs = freq_scale * np.cos(dlat_mesh) * np.sin(dlon_mesh) * c2 + c2 + freq_offset\n","    rs = _radius_km * (1 - np.cos(dlon_mesh) * np.cos(dlat_mesh)) / _row_dist_km + delay_offset\n","\n","    rs = rs.astype('i')\n","    cs = cs.astype('i')\n","    valid = (cs >= 0) & (cs < img_b.shape[1]) & (rs >= 0) & (rs < img_b.shape[0])   \n","    \n","    start_astrotime = SRP['start_astrotime']\n","    SRP_lon_deg = SRP['ephemerides']['PDObsLon'][0]\n","    SRP_lat_deg = SRP['ephemerides']['PDObsLat'][0]\n","\n","    if 0: # No SRP transform: as if the SRP were (0, 0)\n","        addLLV(G, Gc, dlon_mesh[valid], dlat_mesh[valid], img_b[rs[valid], cs[valid]])\n","\n","    if 1: # Transformed to the SRP and rotated by the \"doppler angle\"\n","        # Convert to unit spheroid cartesian coordinates.\n","        cdlat_mesh = np.cos(dlat_mesh)\n","        X = np.matrix((cdlat_mesh * np.cos(dlon_mesh),\n","                      cdlat_mesh * np.sin(dlon_mesh),\n","                      np.sin(dlat_mesh)))\n","\n","        # The S matrix rotates the coordinate system to center the SRP.\n","        clon = np.cos(SRP_lon_deg / 180 * np.pi)\n","        clat = np.cos(SRP_lat_deg / 180 * np.pi)\n","        slon = np.sin(SRP_lon_deg / 180 * np.pi)\n","        slat = np.sin(SRP_lat_deg / 180 * np.pi)\n","        S = np.matrix((( clon * clat,  slon * clat, slat),\n","                       (       -slon,         clon,    0),\n","                       (-clon * slat, -slon * slat, clat))) \n","\n","        ## The D matrix rotates the coordinates about the x axis by the apparent doppler angle:\n","        doppler_ang = apparentRotationAngle_poliastro(start_astrotime) + fudge_deg * au.deg\n","        #doppler_ang = apparentRotationAngle_horizons(start_astrotime) + fudge_deg * au.deg\n","        cnp = np.cos(doppler_ang.to(au.radian))\n","        snp = np.sin(doppler_ang.to(au.radian))\n","        D = np.matrix(((1,   0,    0),\n","                       (0, cnp, -snp),\n","                       (0, snp,  cnp))) \n","\n","        X = (S.T * D.T) * X\n","    \n","        # Convert back to lat/lon\n","        dlat_mesh = np.arcsin(X[2].A)[0]\n","        dlon_mesh = np.arctan2(X[1].A, X[0].A)[0]\n","        addLLV(G, Gc, dlon_mesh[valid], dlat_mesh[valid], img_b[rs[valid], cs[valid]])\n","\n","if _interactive:\n","    G = np.zeros((16000, 8000), dtype='f') # TODO: use a smaller data rep?\n","    Gc = np.zeros(G.shape, 'int')\n","    dopplerDelayToSphericalProjection(img_b, G, Gc, lbl_dict, SRP, best_fit_parameters)\n","\n","    # debug\n","    Gm = np.divide(G, Gc, where=Gc>0)\n","    print(filename)\n","\n","    if 1:\n","        plt.figure(figsize=(20, 10))\n","        plt.axis('off')\n","        plt.imshow(Gm.T[::4, ::4], cmap='gray', origin='lower')\n","        #plt.imshow(G.T[::4, ::4], cmap='gray', origin='lower')\n","        #plt.imshow(G.T, cmap='gray', origin='lower') # , interpolation='none')\n","\n","    if 1:\n","        plt.imsave(f\"{ROOT_PREFIX}/GLOBAL_TRIAGE/{filename[:-4]}_global.png\", Gm.T[::1, ::1], origin='lower')\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["## Full doppler/delay processing pipeline (with caching)\n","from matplotlib import pylab as plt\n","import numpy as np\n","import os\n","import pickle\n","\n","\n","FILTER_PREFIX = ROOT_PREFIX + \"ROLL_GOOD_NEW/\"\n","FILTER = set()\n","filenames = os.listdir(FILTER_PREFIX)\n","for filename in filenames:\n","    FILTER.add(filename[:25])\n","\n","ROLLUP_CACHE = {}\n","#filenames = os.listdir(ROOT_PREFIX + \"ROLLUP_GOOD/\")\n","filenames = os.listdir(ROOT_PREFIX + \"ROLLUP_GOOD_NEW/\")\n","for filename in filenames:\n","    if not filename.endswith('.png'): continue\n","    r = int(filename[33:].split('.')[0])\n","    ROLLUP_CACHE[filename[:25]] = r\n","\n","ROLL_CACHE = {}\n","#filenames = os.listdir(ROOT_PREFIX + \"ROLL_GOOD/\")\n","filenames = os.listdir(ROOT_PREFIX + \"ROLL_GOOD_NEW/\")\n","for filename in filenames:\n","    if not filename.endswith('.png'): continue\n","    r = int(filename[31:].split('.')[0])\n","    ROLL_CACHE[filename[:25]] = r\n","\n","FIT_CACHE = {}\n","filenames = os.listdir(ROOT_PREFIX + \"FIT_GOOD/\")\n","for filename in filenames:\n","    if not filename.endswith('.png'): continue\n","    #params = [float(x) for x in filename[30:].split('.')[0].split(\"_\")]\n","    params = filename[30:-4].split(\"_\")\n","    FIT_CACHE[filename[:25]] = [int(params[0]), int(params[1]), float(params[2])]\n","\n","\n","def processDopplerDelayImage(filename, G, Gc, fudge_deg=0):\n","\n","    if not filename[:25] in FILTER: return\n","\n","    print('Processing', filename)\n","\n","    img = np.memmap(DATA_PREFIX + filename, dtype='<F', shape=(8191, 8192), mode='r')  # 'F' s complex single-precision: a complex number type of 2 32-bit precision floating-point numbers.\n","\n","    lbl_dict = parseLbl(filename)\n","\n","    SRP = pickle.load(open(DATA_PREFIX + filename[:-4] + '_horizons.pkl', 'rb'))\n","\n","    # HACK: 1988 data seems to have flipped doppler?\n","    if lbl_dict['START_TIME'].startswith('1988'): img = np.fliplr(img)\n","\n","    # First do basic image processing: convert to magnitude from complex, and normalize.\n","    img_a = coarsePreprocessDopplerDelay(img)\n","\n","    # Second, coarse tune the range (rollup)\n","    #best_rollup = coarseTuneRollup(img_a)\n","    best_rollup = ROLLUP_CACHE[filename[:25]]  # Use cached rollup\n","    img_a = np.roll(img_a, best_rollup, axis=0)\n","\n","    # Third, fine process (which requires a tuned rollup)\n","    img_b = finePreprocessDopperDelay(img_a)\n","\n","    # Fourth, tune the symmetry (roll)\n","    #best_roll = coarseTuneRoll(img_b, lbl_dict, filename)\n","    best_roll = ROLL_CACHE[filename[:25]]  # Use cached roll\n","    img_b = np.roll(img_b, best_roll)\n","\n","    best_fit_score, best_fit_parameters = fitDopplerDelayCurve(img_b, lbl_dict, filename)\n","    #best_fit_parameters = FIT_CACHE[filename[:25]]  # Use cached params\n","\n","    ## Create new global map image and count image\n","    G = np.zeros((16000, 8000), dtype='f') # TODO: use a smaller data rep?\n","    Gc = np.zeros(G.shape, 'int')\n","\n","    dopplerDelayToSphericalProjection(img_b, G, Gc, lbl_dict, SRP, best_fit_parameters, fudge_deg)\n","\n","    Gm = np.divide(G, Gc, where=Gc>0)\n","    plt.imsave(f\"{ROOT_PREFIX}/GLOBAL_TRIAGE/{filename[:25]}.png\", Gm.T[::1, ::1], origin='lower')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Batch and commandline processing...\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["#processDopplerDelayImage(\"venus_scp_20150811_174505.img\")\n","#processDopplerDelayImage(\"venus_ocp_20150816_171104.img\")\n","#processDopplerDelayImage(\"venus_ocp_19880617_151830.img\")\n","\n","if 0: # Test global combination\n","    file1 = \"venus_scp_20150815_170058.img\"\n","    #file1 = \"venus_ocp_19880617_150830.img\"\n","    file2 = \"venus_scp_20150812_155242.img\" \n","    #file3 = \"venus_scp_20170324_171055.img\"  # Appears to be ~0.2 degrees off?\n","    for fudge_deg in (0.0,):\n","    #for fudge_deg in (-0.5, -0.6):\n","    #for fudge_deg in (0.1, -0.1, -0.2, -0.3, -0.4, -0.7):\n","    #for fudge_deg in (-2, -1, 0, 1, 2):\n","        G = np.zeros((16000, 8000), dtype='f') # TODO: use a smaller data rep?\n","        Gc = np.zeros(G.shape, 'int')\n","        processDopplerDelayImage(file1, G, Gc)\n","        processDopplerDelayImage(file2, G, Gc, fudge_deg)\n","        Gm = np.divide(G, Gc, where=Gc>0)\n","        plt.imsave(f\"{ROOT_PREFIX}/GLOBAL_TRIAGE/pair_{file1[:25]}_{file2[:25]}_fudge_{fudge_deg}.png\", Gm.T[::1, ::1], origin='lower')"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["if 0: ## Batch processing in notebook\n","    import os\n","    \n","    for filename in os.listdir(DATA_PREFIX):\n","        if not filename.endswith('.img'): continue\n","        #if not '_2012' in filename: continue\n","        if not '_2001' in filename: continue\n","        processDopplerDelayImage(filename)\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["## Main function to allow being run from the command-line.\n","import sys\n","\n","if __name__ == '__main__' and \"get_ipython\" not in dir():  # Not imported, not run from Jupyter/IPython\n","    args = sys.argv[1:]\n","    #print('args', args)\n","    ## Create new global map image and count image\n","    G = np.zeros((16000, 8000), dtype='f') # TODO: use a smaller data rep?\n","    Gc = np.zeros(G.shape, 'int')\n","    processDopplerDelayImage(args[0], G, Gc)\n","    sys.exit()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/mnt/c/Users/natha/code/venera\n","[NbConvertApp] Converting notebook Venera - Radar.ipynb to python\n","[NbConvertApp] Writing 34550 bytes to Venera - Radar.py\n","/mnt/c/Users/natha/Downloads/venus/arecibo_radar/pds-geosciences.wustl.edu/venus/arcb_nrao-v-rtls_gbt-3-delaydoppler-v1/vrm_90xx/data\n","^C\n"]}],"source":["## Parallel magic.\n","\n","## 1. Convert to .py with (from \"My Drive/Colab Notebooks\" directory):\n","#%cd \"/mnt/g/My Drive/Colab Notebooks\"\n","%cd \"/mnt/c/Users/natha/code/venera\"\n","! jupyter nbconvert --to python Venera\\ -\\ Radar.ipynb\n","\n","## 2. Run the .py in parallel (from the venus data directory):\n","%cd $DATA_PREFIX\n","#! ls -1 *2017*.img | xargs -n 1 -P 4 python3 /home/than/My\\ Drive/Colab\\ Notebooks/Venera\\ -\\ Radar.py\n","! ls -1 *.img | xargs -n 1 -P 4 python3 /mnt/c/Users/natha/code/venera/Venera\\ -\\ Radar.py"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPzhu/Sb46Ue9ZfoPLkzO03","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"40fefee005f43b138c30243dc2ffca6e84360bada605f47b1fde319709267cca"}}},"nbformat":4,"nbformat_minor":0}
