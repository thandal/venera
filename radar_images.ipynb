{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Arecibo Planetary Radar Doppler/Delay Image Processing"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vcDc77Oq_21F"},"source":["References:\n","* https://iopscience.iop.org/article/10.3847/PSJ/ac4f43/pdf -- *Arecibo Radar Maps of Venus from 1988 to 2020* (overview paper)\n","* https://pds-geosciences.wustl.edu/venus/arcb_nrao-v-rtls_gbt-3-delaydoppler-v1/vrm_90xx/document/venus_radar.pdf -- Level 1 doppler/dely data description\n","* https://pds-geosciences.wustl.edu/venus/urn-nasa-pds-venus_radar_level2/document/venus_radar_maps_user_guide.pdf -- Level 2 multi-look data description\n","* https://pubs.usgs.gov/of/1993/0516/report.pdf -- Venus Geologic Mappers' Handbook\n","* https://repository.si.edu/bitstream/handle/10088/3303/200737.pdf -- *Focused 70-cm wavelength radar mapping of the moon* Details on the doppler/delay transformations\n","* https://echo.jpl.nasa.gov/asteroids/harmon.2002.long.code.pdf -- Details on the long-code doppler-delay method\n","\n","Tools/Libraries:\n","* https://ssd.jpl.nasa.gov/horizons/app.html#/ -- JPL Horizons web app\n","* https://astroquery.readthedocs.io/en/latest/jplhorizons/jplhorizons.html -- JPL Horizons Python API\n","* https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/planets/de440_and_de441.pdf -- Latest ephemerides description\n","* https://astropedia.astrogeology.usgs.gov/download/Docs/WGCCRE/WGCCRE2009reprint.pdf --  Report of the IAU Working Group on Cartographic Coordinates and Rotational Elements: 2009\n","* https://escholarship.org/content/qt2dn1012s/qt2dn1012s.pdf?t=pv3anr -- The mean rotation rate of Venus from 29â€¯years of Earth-based radar observations (also describes doppler/delay transformation)\n","\n","Maps:\n","* http://134.158.75.177/viewer/Apps/PlanetaryCesiumViewer/index.html?view=0%2C0%2C35255004%2C360%2C-90%2C0 -- with Venus lat/lon, etc.\n","* https://www.google.com/maps/space/venus/\n","* https://en.wikipedia.org/wiki/Mapping_of_Venus\n","* https://solarsystem.nasa.gov/resources/2342/venus-surface-3d-model/"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["from matplotlib import pyplot as pl\n","import numpy as np\n","import os\n","import pickle"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Apparent Rotation Angle (Doppler Angle) Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: Poliastro-based doppler angle calculation\n","from astropy import units as au\n","from astropy import coordinates as ac\n","from astropy import constants as ak\n","from astropy import time as at\n","from poliastro.bodies import Venus\n","from poliastro.frames.fixed import VenusFixed\n","from poliastro.frames.equatorial import VenusICRS\n","from poliastro.spheroid_location import SpheroidLocation\n","\n","# NOTE: You MUST use poliastro >=0.18 to avoid a bug in VenusFixed coordinates! (see https://github.com/poliastro/poliastro/issues/1599)\n","\n","# Set high quality ephemerides\n","# https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/planets/de440_and_de441.pdf appears to recommend de440s for our purposes:\n","# - de440s : 1849 to 2150 (32 MB)\n","# - de440 : 1550 to 2650 (114 MB)\n","# - de441 : -13200 to 17191(3.2 GB)\n","ac.solar_system_ephemeris.set(\"de440s\")  # JPL Horizons is using de441, which is in the same de44X family. \n","\n","def apparentRotationAngle_poliastro(obstime):\n","    # Observer point (Arecibo) at observation time in ICRS frame\n","    o_coord = ac.EarthLocation.of_site('arecibo')\n","    o_gcrs = o_coord.get_gcrs(obstime)\n","    o_icrs = o_gcrs.transform_to(ac.ICRS())\n","\n","    # Venus Body Center\n","    vBC_fixed = VenusFixed(obstime=obstime,\n","                           x=0 * au.m, y=0 * au.m, z=0 * au.m,\n","                           v_x=0 * au.m / au.s, v_y=0 * au.m / au.s, v_z=0 * au.m / au.s,\n","                           representation_type='cartesian', differential_type='cartesian')\n","\n","    # Venus North Pole\n","    v = SpheroidLocation(0.0 * au.deg, 90.0 * au.deg, 0.0 * au.m, Venus)\n","    x, y, z = v.cartesian_cords\n","    vNP_fixed = VenusFixed(obstime=obstime,\n","                           x=x, y=y, z=z,\n","                           v_x=0 * au.m / au.s, v_y=0 * au.m / au.s, v_z=0 * au.m / au.s,\n","                           representation_type='cartesian', differential_type='cartesian')\n","\n","    VenusICRS(obstime=obstime) # WORKAROUND -- for some reason we have to do this once (see https://github.com/poliastro/poliastro/issues/1598)\n","\n","    vBC_o_icrs = vBC_fixed.transform_to(o_icrs)\n","    vNP_o_icrs = vNP_fixed.transform_to(o_icrs)\n","\n","    # North Pole axis (in Arecibo frame)\n","    axis_NP = vNP_o_icrs.cartesian.without_differentials() - vBC_o_icrs.cartesian.without_differentials()\n","    axis_NP /= axis_NP.norm()\n","\n","    # Apparent Rotation axis (in Arecibo frame)\n","    axis_BC = vBC_o_icrs.cartesian.without_differentials() - o_icrs.cartesian.without_differentials()\n","    axis_BC /= axis_BC.norm()\n","    axis_AR = axis_BC.cross(vBC_o_icrs.velocity - o_icrs.velocity)\n","    axis_AR /= axis_AR.norm()\n","\n","    # Project the North Pole axis onto the plane of the sky\n","    sky_NP = axis_BC.cross(axis_NP.cross(axis_BC))\n","    sky_NP /= sky_NP.norm()\n","\n","    # Project the Apparent Rotation axis onto the plane of the sky\n","    sky_AR = axis_BC.cross(axis_AR.cross(axis_BC))\n","    sky_AR /= sky_AR.norm()\n","\n","    # Calculate the angle difference between the two sky-projected axes, bounded from -90 to 90 degrees.\n","    sky_delta_angle = np.arccos(sky_NP.dot(sky_AR) / (sky_NP.norm() * sky_AR.norm()))\n","    if sky_delta_angle > 90 * au.deg: sky_delta_angle -= 180 * au.deg\n","    if sky_delta_angle < -90 * au.deg: sky_delta_angle += 180 * au.deg\n","    return sky_delta_angle\n","\n","def orthogonalVelocity_poliastro(obstime):\n","    # Observer point (Arecibo) at observation time in ICRS frame\n","    o_coord = ac.EarthLocation.of_site('arecibo')\n","    o_gcrs = o_coord.get_gcrs(obstime)\n","    o_icrs = o_gcrs.transform_to(ac.ICRS())\n","\n","    # Venus Body Center\n","    vBC_fixed = VenusFixed(obstime=obstime,\n","                           x=0 * au.m, y=0 * au.m, z=0 * au.m,\n","                           v_x=0 * au.m / au.s, v_y=0 * au.m / au.s, v_z=0 * au.m / au.s,\n","                           representation_type='cartesian', differential_type='cartesian')\n","    VenusICRS(obstime=obstime) # WORKAROUND\n","    vBC_o_icrs = vBC_fixed.transform_to(o_icrs)\n","\n","    dpos = vBC_o_icrs.cartesian.without_differentials() - o_icrs.cartesian.without_differentials()\n","    dvel = vBC_o_icrs.velocity - o_icrs.velocity\n","\n","    dpos_m = dpos.xyz.to(au.m).value\n","    range_m = np.sqrt(np.sum(dpos_m**2))\n","    dvel_mps = dvel.d_xyz.to(au.m / au.s).value\n","\n","    # \"Radial\" and \"Orthogonal\" velocity\n","    range_rate_mps = np.dot(dvel_mps, dpos_m / range_m)\n","    ortho_mps = np.sqrt(sum(dvel_mps**2) - range_rate_mps**2)\n","    #print(f'{range_m=}')\n","    #print(f'{range_rate_mps=}')\n","    #print(f'{ortho_mps=}')\n","    return ortho_mps"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: JPL Horizons-based doppler angle calculation\n","from astropy import coordinates as ac\n","from astroquery.jplhorizons import Horizons\n","\n","def apparentRotationAngle_horizons(obstime):\n","    # Venus points in Arecibo (-1 @ 399) frame:\n","    # Venus Body Center\n","    vBC = Horizons(id='299',  location='-1 @ 399', epochs=obstime.tdb.jd).vectors()\n","    # Venus North Pole\n","    vNP = Horizons(id='g: 0, 90, 0 @ 299',  location='-1 @ 399', epochs=obstime.tdb.jd).vectors()\n","\n","    vBC_pos = ac.CartesianRepresentation(\n","        x=vBC['x'][0] * vBC['x'].unit,\n","        y=vBC['y'][0] * vBC['y'].unit,\n","        z=vBC['z'][0] * vBC['z'].unit)\n","    vBC_vel = ac.CartesianRepresentation(\n","        x=vBC['vx'][0] * vBC['vx'].unit,\n","        y=vBC['vy'][0] * vBC['vy'].unit,\n","        z=vBC['vz'][0] * vBC['vz'].unit)\n","\n","    vNP_pos = ac.CartesianRepresentation(\n","        x=vNP['x'][0] * vNP['x'].unit,\n","        y=vNP['y'][0] * vNP['y'].unit,\n","        z=vNP['z'][0] * vNP['z'].unit)\n","    vNP_vel = ac.CartesianRepresentation(\n","        x=vNP['vx'][0] * vNP['vx'].unit,\n","        y=vNP['vy'][0] * vNP['vy'].unit,\n","        z=vNP['vz'][0] * vNP['vz'].unit)\n","\n","    # North Pole axis\n","    axis_NP = vNP_pos - vBC_pos\n","    axis_NP /= axis_NP.norm()\n","\n","    # Apparent Rotation axis\n","    axis_BC = vBC_pos\n","    axis_BC /= axis_BC.norm()\n","    axis_AR = axis_BC.cross(vBC_vel)\n","    axis_AR /= axis_AR.norm()\n","\n","    # Project the North Pole axis onto the plane of the sky\n","    sky_NP = axis_BC.cross(axis_NP.cross(axis_BC))\n","    sky_NP /= sky_NP.norm()\n","\n","    # Project the Apparent Rotation axis onto the plane of the sky\n","    sky_AR = axis_BC.cross(axis_AR.cross(axis_BC))\n","    sky_AR /= sky_AR.norm()\n","\n","    # Calculate the angle difference the two sky-projected axes\n","    sky_delta_angle = np.arccos(sky_NP.dot(sky_AR) / (sky_NP.norm() * sky_AR.norm()))\n","    if sky_delta_angle > 90 * au.deg: sky_delta_angle -= 180 * au.deg\n","    if sky_delta_angle < -90 * au.deg: sky_delta_angle += 180 * au.deg\n","    return sky_delta_angle"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Interactive script magic"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["_interactive = True  # Skip the next cell if you want to run in interactive (cell-by-cell) mode!"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["_interactive = False  # Automatically override _interactive to false if this notebook is run in batch mode!"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Main doppler/delay processing pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["ROOT_PREFIX = \"data_venus/arecibo_radar/pds-geosciences.wustl.edu/venus/arcb_nrao-v-rtls_gbt-3-delaydoppler-v1/vrm_90xx/\"\n","DATA_PREFIX = ROOT_PREFIX + \"data/\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669567231541,"user":{"displayName":"Nathaniel Fairfield","userId":"09088445613731179845"},"user_tz":300},"id":"tXBpaMJZbF03","vscode":{"languageId":"python"}},"outputs":[],"source":["if _interactive: ## Select filename\n","    # 1988:\n","    #filename = \"venus_ocp_19880604_163910.img\"  # First S (ever!)\n","    #filename = \"venus_ocp_19880620_144220.img\"\n","    #filename = \"venus_ocp_19880618_150330.img\"\n","    #filename = \"venus_ocp_19880617_151830.img\"\n","\n","    # 2001:\n","    #filename = \"venus_ocp_20010331_160048.img\"\n","    #filename = \"venus_ocp_20010331_184848.lbl\"\n","\n","    # 2012: used GBT, maybe avoid for now due to more complex antenna configuration\n","\n","    # 2015:\n","    #filename = \"venus_ocp_20150810_162629.img\"  # First S\n","    #filename = \"venus_ocp_20150810_163623.img\"  # Next S\n","    #filename = \"venus_ocp_20150816_171104.img\"  # Last S\n","    #filename = \"venus_ocp_20150812_155242.img\"  # First N\n","    #filename = \"venus_scp_20150815_172030.img\"  # Last N\n","    filename = \"venus_ocp_20150812_155242.img\" # Bruce parameter comparison\n","    #filename = \"venus_ocp_20150815_170058.img\" # Bruce parameter comparison\n","\n","    # 2017:\n","\n","    # 2020:\n","    #filename = \"venus_scp_20200530_175421.img\"\n","    #filename = \"venus_ocp_20200530_175421.img\"\n","    #filename = \"venus_ocp_20200530_174429.img\"\n","    #filename = \"venus_ocp_20200530_173437.img\"\n","    #filename = \"venus_ocp_20200530_172445.img\"\n","\n","    # Vertical roll experiments\n","    #filename = 'venus_ocp_20010331_163648.img'  # Actually needs to be rolled *down* by a small amount\n","    #filename = 'venus_ocp_20120527_180159.img'  # Needs to be rolled up significantly\n","    #filename = 'venus_ocp_19880605_175820.img'  # Weak signal?\n","    #filename = 'venus_scp_19880604_163910.img'   # weak\n","    #filename = 'venus_scp_20200524_182906.img' #  weakish\n","\n","    # Fit experiments\n","    #filename = 'venus_ocp_19880604_163910.img'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: Parse the .lbl metadata file\n","def parseLbl(filename):\n","    lbl_filepath = DATA_PREFIX + filename[:25] + \".lbl\"\n","    l = {}\n","    for line in open(lbl_filepath).readlines():\n","        if 'START_TIME' in line:\n","            l['START_TIME'] = line.split('=')[-1].strip()\n","        if 'STOP_TIME' in line:\n","            l['STOP_TIME'] = line.split('=')[-1].strip()\n","        if 'GEO:BAUD' in line:\n","            l['GEO_BAUD'] = float(line.split(' ')[-2])\n","        if 'GEO:CENTROID_LOCATION' in line:\n","            l['GEO_CENTROID_LOCATION'] = int(line.split('=')[1].strip())\n","        if 'GEO:DELAY_OFFSET' in line:\n","            l['GEO_DELAY_OFFSET'] = int(line.split('=')[1].strip())\n","        if 'GEO:POINTING' in line:\n","            l['GEO_POINTING'] = line.split('\"')[-2]\n","        if 'GEO:MODE' in line:\n","            l['GEO_MODE'] = line.split('\"')[-2]\n","    return l\n","\n","\n","if _interactive: \n","    lbl_dict = parseLbl(filename)\n","    print(f'{lbl_dict=}')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: Load/fetch Horizons ephemeris data\n","from astropy import time as at\n","from astroquery.jplhorizons import Horizons\n","\n","def loadOrFetchHorizonsData(filename):\n","    cache_filepath = DATA_PREFIX + filename[:25] + '_horizons.pkl'\n","    if os.path.exists(cache_filepath):\n","      return pickle.load(open(cache_filepath, 'rb'))\n","    # Cache miss: query JPL Horizons\n","    lbl_dict = parseLbl(f)\n","    start_astrotime = at.Time(lbl_dict['START_TIME'])\n","    stop_astrotime = at.Time(lbl_dict['STOP_TIME'])\n","    obj = Horizons(id='299',  # Venus\n","                   location='-1@399',  # Arecibo on Earth\n","                   epochs=[start_astrotime.jd, stop_astrotime.jd])\n","    hdata = {\n","          'start_astrotime': start_astrotime,\n","          'stop_astrotime': stop_astrotime,\n","          'ephemerides': obj.ephemerides(),\n","          'vectors': obj.vectors(),\n","        }\n","    pickle.dump(hdata, open(cache_filepath, 'wb'))\n","    return hdata\n","\n","\n","if 0: # Batch cache: Lookup and cache ALL the JPL Horizons ephemerides data.\n","    for f in os.listdir(DATA_PREFIX):\n","        if not f.endswith('.img'): continue\n","        loadOrFetchHorizonsData(f)\n","\n","\n","if _interactive: # Load sub-radar point (SRP) data\n","    hdata = loadOrFetchHorizonsData(filename)\n","    start_astrotime = hdata['start_astrotime']\n","    end_astrotime = hdata['start_astrotime']\n","    eph = hdata['ephemerides']\n","    srp_lon_deg = eph['PDObsLon'][0]\n","    srp_lat_deg = eph['PDObsLat'][0]\n","    if 1:  # debug\n","        print(f\"{start_astrotime=}\")\n","        print(f\"{srp_lon_deg=}\")\n","        print(f\"{srp_lat_deg=}\")\n","        print(f\"{eph['NPole_ang'][0]=}\")\n","    if 1:  # debug\n","        sky_delta_angle_poliastro = apparentRotationAngle_poliastro(start_astrotime)\n","        print(f'{sky_delta_angle_poliastro.to(au.deg).value=}')\n","        #sky_delta_angle_horizons = apparentRotationAngle_horizons(start_astrotime)\n","        #print(f'{sky_delta_angle_horizons.to(au.deg).value=}')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["if 0: # debug: Compare ephemeris data\n","    YEAR = []\n","    NP_ANG_DEG = []\n","    NP_DIS_ARCSEC = []\n","    SRP_LON_DEG = []\n","    SRP_LAT_DEG = []\n","    DOP_ANG_DEG = []\n","    AZ_DEG = []\n","    EL_DEG = []\n","    for f in os.listdir(DATA_PREFIX):\n","        #if not '1988' in f: continue\n","        #if not '2015' in f: continue\n","        #if not '2017' in f: continue\n","        #if not '2020' in f: continue\n","        if not f.endswith('horizons.pkl'): continue\n","        lbl_dict = parseLbl(f)\n","        if lbl_dict['GEO_POINTING'] != \"N\": continue\n","        YEAR.append(lbl_dict['START_TIME'][:4])\n","        hdata = loadOrFetchHorizonsData(f)\n","        eph = hdata['ephemerides']\n","        NP_ANG_DEG.append(eph['NPole_ang'][0])\n","        NP_DIS_ARCSEC.append(eph['NPole_dist'][0])\n","        SRP_LON_DEG.append(eph['PDObsLon'][0])\n","        SRP_LAT_DEG.append(eph['PDObsLat'][0])\n","        AZ_DEG.append(eph['AZ'][0])\n","        EL_DEG.append(eph['EL'][0])\n","        if 1: # debug: Compare apparent rotation angles\n","            sky_delta_angle_poliastro = apparentRotationAngle_poliastro(hdata['start_astrotime'])\n","            #sky_delta_angle_horizons = apparentRotationAngle_horizons(hdata['start_astrotime'])\n","            #print(f'{sky_delta_angle_poliastro.to(au.deg)=}')\n","            #print(f'{sky_delta_angle_horizons.to(au.deg)=}')\n","            DOP_ANG_DEG.append(sky_delta_angle_poliastro.to(au.deg).value)\n","            #DOP_ANG_DEG.append(sky_delta_angle_horizons.to(au.deg).value)\n","        #print(SRP['start_astrotime'], eph['PDObsLon'][0], eph['PDObsLat'][0])\n","    print(len(SRP_LON_DEG))\n","    print(np.mean(SRP_LON_DEG))\n","    print(np.mean(SRP_LAT_DEG))\n","\n","if 1: # debug\n","    #pl.plot(NP_DIS_ARCSEC)\n","    #pl.plot(NP_ANG_DEG)\n","    #pl.plot(DOP_ANG_DEG)\n","    #NP_ANG = np.array(NP_ANG)\n","    #pl.plot(np.where(NP_ANG > 180, NP_ANG - 360, NP_ANG))\n","    #pl.plot(SRP_LAT)\n","\n","    # Sanity check: Campbell says that doppler angles should be ~10 degrees!\n","    # ??? -- many of these are up to 20 degrees! Perhaps he wasn't looking at 1988, 2012, or 2020?\n","    pl.figure()\n","    pl.hist(DOP_ANG_DEG)\n","    pl.title('Histogram of doppler angles (degrees)')\n","\n","    pl.figure()\n","    pl.plot(YEAR, DOP_ANG_DEG, '.')\n","    pl.title('Doppler angles by year')\n","\n","    # Sanity check: sub radar points should be within ~10 degrees of the Venusian equator\n","    pl.figure()\n","    pl.plot(SRP_LON_DEG, SRP_LAT_DEG, \".\")\n","    pl.axis('scaled')\n","    pl.title('Sub radar point location')\n","    pl.xlabel('longitude (degrees)') \n","    pl.ylabel('latitude (degrees)') \n","\n","    # Sanity check: elevation should be close to the zenith for Arecibo!\n","    pl.figure()\n","    pl.plot(AZ_DEG, EL_DEG, '.')\n","    pl.title(\"Az/El\")"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["if _interactive:  # Load doppler delay image\n","    # File format an 8191 * 8192 array of pixels, each pixel is a complex number represented by real, imaginary components stored as single-precision floats.\n","    img = np.memmap(DATA_PREFIX + filename, dtype=\"<F\", shape=(8191, 8192), mode=\"r\")  # 'F' is complex single-precision: a complex number type of 2 32-bit precision floating-point numbers.\n","    print(f\"Loaded {filename}\")\n","    print(f\"{img.shape=}\")\n","    \n","    # HACK: 1988 and 2020 data seems to have flipped doppler!?\n","    if lbl_dict['START_TIME'].startswith('1988') or lbl_dict['START_TIME'].startswith('2020'):\n","        img = np.fliplr(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: first step of doppler delay image processing pipeline\n","def coarsePreprocessDopplerDelay(img):\n","    c2 = img.shape[1] // 2\n","    ## Convert complex valued image to magnitude image.\n","    img_a = np.abs(img)\n","    ## Normalize the image    \n","    img_a -= img_a.min()\n","    img_a /= img_a.max()\n","    ## Roll the frequency axis (left/right) by the default so that zero doppler is in the middle of the image, and the\n","    # planet looks like a planet.\n","    # NOTE: this is NOT a fine-tuned fit but should be correct to within ~100 columns.\n","    img_a = np.roll(img_a, c2)\n","    return img_a\n","\n","\n","if _interactive:\n","    img_a = coarsePreprocessDopplerDelay(img)\n","\n","    if 0: # debug -- pretty dark until further processing!\n","        #pl.imsave(filename + \".png\", img_a)\n","        pl.figure(figsize=(16, 16))\n","        pl.axis('off')\n","        pl.imshow(img_a, cmap='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: second step of doppler delay image processing pipeline\n","def coarseTuneRollup(img_a):\n","    c2 = img_a.shape[1] // 2\n","    ## Roll the range axis (up/down) so that SRP onset starts at row 0\n","    # First, identify the biggest peak in the center-slice of the image.\n","    c = np.sum(img_a[:, 3500:-3500], axis=1)\n","    #plt.plot(c, 'r-')\n","    max_c = np.argmax(c)\n","    # Center the biggest peak to avoid wrapping issues.\n","    center_i = c2 - max_c\n","    c = np.roll(c, center_i)\n","    # Find the first big increase above the noise floor\n","    d = np.diff(c[c2 - 200:c2 + 1])\n","    #plt.plot(d, 'b-')\n","    d_pre_std = np.std(d[:c2 - 100])  # std from samples *before* the spike\n","    first_i = np.argwhere(d > d_pre_std * 2)[0][0]\n","    # Roll the image up/down to put first onset at row 0\n","    return -(max_c - 200 + first_i) - 1\n","\n","\n","if _interactive:\n","    coarse_rollup = coarseTuneRollup(img_a)\n","    print(f\"{coarse_rollup=}\")\n","    print(f\"{lbl_dict['GEO_DELAY_OFFSET']=}\")\n","    img_a = np.roll(img_a, coarse_rollup, axis=0)\n","\n","    if 0: # debug -- pretty dark until further processing!\n","        #pl.imsave(filename + \".png\", img_a)\n","        pl.figure(figsize=(16, 16))\n","        pl.axis('off')\n","        pl.imshow(img_a, cmap='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: third step of doppler delay image processing pipeline\n","def finePreprocessDopperDelay(img_a):\n","    # TODO: improve the echo power model -- AND map it properly to the angle of incidence!\n","    # TODO: apply a different rule for OCP? \n","    echo_power = 1 - np.cos(np.linspace(0, np.pi / 2, img_a.shape[0])) + 1e-1  # Add a litte to avoid zero.\n","    img_b = (img_a.T * echo_power).T\n","\n","    # TODO: \"noise normalize\"? (Mentioned in the papers)\n","    #noise_patch = img[:1000,:1000]\n","    #average_noise = np.mean(np.abs(noise_patch))\n","    #print('average_noise', average_noise)\n","    #img_b = img_a.copy()\n","    #img_b -= average_noise\n","\n","    # Clip the 99th percentile pixel brightness\n","    percentile = 99\n","    percentile_thresh = np.percentile(img_b.ravel(), percentile)\n","    img_b = np.where(img_b > percentile_thresh, percentile_thresh, img_b)\n","\n","    #percentile = 5\n","    #percentile_thresh = np.percentile(img_b.ravel(), percentile)\n","    #img_b = np.where(img_b < percentile_thresh, percentile_thresh, img_b)\n","    #if filename: plt.imsave(f\"{ROOT_PREFIX}/PREPROCESS_TRIAGE/{filename[:-4]}_processed.png\", img_b, cmap=\"gray\")\n","    return img_b\n","\n","\n","if _interactive:\n","    img_b = finePreprocessDopperDelay(img_a)\n","\n","    if 1:  # debug -- image should look pretty good!\n","        #pl.imsave(filename + \".png\", img_b)\n","        pl.figure(figsize=(16, 16))\n","        pl.axis('off')\n","        pl.imshow(img_b, cmap='gray')\n","\n","    if 1:  # debug brightness histogram\n","        pl.figure(figsize=(16, 4))\n","        pl.hist(img_b.ravel(), bins=50)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: forth step of doppler delay image processing pipeline\n","def coarseTuneRoll(img_b, lbl_dict, filename=None): # Coarse-tune the centering by rolling to maximize left-right symmetry\n","    best_col_offset = 0\n","    best_col_offset_sum = 0\n","    # Baud roughly determines the frequency spreading, so we use different windows to avoid the wrap region.\n","    if lbl_dict[\"GEO_BAUD\"] == 4.2:\n","        img_h = img_b[2500:3000, :].copy()\n","    if lbl_dict[\"GEO_BAUD\"] == 4.0:\n","        img_h = img_b[3000:4000, :].copy()\n","    elif lbl_dict[\"GEO_BAUD\"] == 3.9:\n","        img_h = img_b[4000:5000, :].copy()\n","    elif lbl_dict[\"GEO_BAUD\"] == 3.8:  # For 2012\n","        img_h = img_b[3500:4500, :].copy()\n","    else:\n","        raise(\"Unknown GEO_BAUD\", lbl_dict[\"GEO_BAUD\"])\n","    for offset in range(-100, 101):  # HUGE range -- slow but necessary?\n","    #for offset in range(-50, 51):  # More sane range?\n","        img_tmp = np.roll(img_h, offset)\n","        #img_tmp = np.diff(img_tmp, axis=1, prepend=0)**2\n","        img_mirror = img_tmp[:, :1000] * np.fliplr(img_tmp[:, -1000:])\n","        #img_mirror = np.diff(img_tmp[:, :1000], axis=1)**2 * np.diff(np.fliplr(img_tmp[:, -1000:]), axis=1)**2  # Difference image\n","        #img_mirror = img_tmp * np.fliplr(img_tmp)\n","        #plt.imsave(filename + '_roll' + str(offset) + '.png', img_mirror)\n","        #plt.imshow(img_mirror, cmap='gray')\n","        total_sum = np.sum(img_mirror)\n","        #print(offset, total_sum)\n","        if total_sum > best_col_offset_sum:\n","            best_col_offset_sum = total_sum\n","            best_col_offset = offset\n","    if filename: # save a debug file, which also can be used to later look up the best roll\n","        img_tmp = np.roll(img_h, best_col_offset)\n","        img_mirror = img_tmp[:, :1000] * np.fliplr(img_tmp[:, -1000:])\n","        os.makedirs(f\"{ROOT_PREFIX}/ROLL_TRIAGE\", exist_ok=True)\n","        pl.imsave(f\"{ROOT_PREFIX}/ROLL_TRIAGE/{filename[:-4]}_roll_{best_col_offset}.png\", img_mirror)\n","    return best_col_offset\n","    \n","if _interactive:\n","    best_col_offset = coarseTuneRoll(img_b, lbl_dict)\n","    print(f'{best_col_offset=}')\n","    img_b = np.roll(img_b, best_col_offset)\n","\n","    if 1: # debug\n","        #plt.imsave(filename + \".png\", img_b)\n","        pl.figure(figsize=(16, 16))\n","        pl.axis('off')\n","        pl.imshow(img_b + np.fliplr(img_b), cmap='gray')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: fit parameters to the doppler/delay image curve\n","\n","def curveRowsAndCols(img_b, freq_offset, delay_offset, freq_scale, c2, sin_dlon, row_dist):\n","    # Utility function\n","    cs = (c2 * freq_scale) * sin_dlon + (c2 + freq_offset)\n","    rs = row_dist + delay_offset\n","    rs = rs.astype('i')\n","    cs = cs.astype('i')\n","    \n","    ### Simple offset\n","    #rs2 = rs\n","    #cs2 = np.where(cs < c2, cs - 2, cs + 2)  # TODO: smarter kernel?\n","\n","    # Orthogonal offset\n","    drs = np.diff(rs, append=rs[-1])\n","    dcs = np.diff(cs, append=cs[-1])\n","    cs2 = cs + np.clip(drs, -1, 1)\n","    rs2 = rs - np.clip(dcs, -1, 1)\n","\n","    rs2 = rs2.astype('i')\n","    cs2 = cs2.astype('i')\n","    return rs, cs, rs2, cs2\n","\n","\n","def fitDopplerDelayCurve(img_b, lbl_dict, filename=None):\n","    c2 = img_b.shape[1] / 2\n","    _radius_km = 6051.8\n","    _row_dist_km = 299792.46 * lbl_dict['GEO_BAUD'] * 1e-6 / 2 # km  Note: row is *round-trip-time* (double distance)\n","    dlon = np.linspace(-np.pi/2, np.pi/2, 8000)\n","    row_dist = _radius_km * (1 - np.cos(dlon)) / _row_dist_km\n","    sin_dlon = np.sin(dlon)\n","\n","    # Grid search by looking for the best \"edge\"\n","    best_score = -np.inf\n","    best_params = []\n","    #for freq_offset in range(-3, 4):  # Small range because image has already been coarsely centered.\n","    #    for delay_offset in range(-3, 4):  # Small range because image has already been coarsely ranged.\n","    for freq_offset in range(-10, 11):  # larger range because image has already been coarsely centered.\n","        for delay_offset in range(-10, 11):  # larger range because image has already been coarsely ranged.\n","            for freq_scale in np.linspace(0.9, 1.4, 301):\n","                rs, cs, rs2, cs2 = curveRowsAndCols(img_b, freq_offset, delay_offset, freq_scale, c2, sin_dlon, row_dist)\n","                row_start = 500 # NOTE: start at row_start to avoid bias due to extremely bright SRP return\n","                row_end = 8000  # NOTE: end at row_end to avoid noise due to frequency wrapping\n","                valid = (cs >= 0) & (cs < img_b.shape[1]) & (rs >= row_start) & (rs < row_end)\n","                valid2 = (cs2 >= 0) & (cs2 < img_b.shape[1]) & (rs2 >= row_start) & (rs2 < row_end)\n","                score = sum(img_b[rs[valid], cs[valid]]) - sum(img_b[rs2[valid2], cs2[valid2]])\n","                if score > best_score:\n","                    best_score = score\n","                    best_params = [freq_offset, delay_offset, freq_scale]\n","    if filename: # save a debug file, which can also be used to later look up the fit parameters\n","        freq_offset, delay_offset, freq_scale = best_params\n","        rs, cs, rs2, cs2 = curveRowsAndCols(img_b, freq_offset, delay_offset, freq_scale, c2, sin_dlon, row_dist)\n","        valid = (cs >= 0) & (cs < img_b.shape[1]) & (rs >= 0) & (rs < img_b.shape[0]) \n","        valid2 = (cs2 >= 0) & (cs2 < img_b.shape[1]) & (rs2 >= 0) & (rs2 < img_b.shape[0])   \n","        img_h = img_b.copy() # Scratch image\n","        img_h[rs2[valid2], cs2[valid2]] = img_h.min() / 2\n","        img_h[rs[valid], cs[valid]] = img_h.max() * 2\n","        os.makedirs(f\"{ROOT_PREFIX}/FIT_TRIAGE_3\", exist_ok=True)\n","        pl.imsave(f\"{ROOT_PREFIX}/FIT_TRIAGE_3/{filename[:-4]}_fit_{freq_offset}_{delay_offset}_{freq_scale}.png\", img_h, cmap='gray')\n","    return best_score, best_params\n","\n","\n","if _interactive:\n","    best_fit_score, best_fit_parameters = fitDopplerDelayCurve(img_b, lbl_dict, \"interactive_\" + filename)\n","    freq_offset, delay_offset, freq_scale = best_fit_parameters\n","    print(f'Best fit score: {best_fit_score:.4f}')\n","    print(f'Best fit parameters: {freq_offset}, {delay_offset}, {freq_scale:.4f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Function: spherical to planar projection utility functions\n","\n","def setLLV(G, Gc, lon, lat, v):\n","    # lon in [0, 2*pi)\n","    # lat in [-pi/2, pi/2]\n","    r = (lon / (2 * np.pi) * G.shape[0]).astype('i')\n","    c = ((lat + np.pi / 2) / np.pi * G.shape[1]).astype('i')\n","    G[r, c] = v\n","    Gc[r, c] = 1\n","\n","def addLLV(G, Gc, lon, lat, v):\n","    # lon in [0, 2*pi)\n","    # lat in [-pi/2, pi/2]\n","    r = (lon / (2 * np.pi) * G.shape[0]).astype('i')\n","    c = ((lat + np.pi / 2) / np.pi * G.shape[1]).astype('i')\n","    G[r, c] += v\n","    Gc[r, c] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["if 0:  # debug: Draw debug lines of latitude and longitude\n","    for lon in range(360):\n","        setLL(G0, lon, 0, 1)\n","        setLL(G0, lon, 45, 0.5)\n","        setLL(G0, lon, -45, 0.2)\n","    \n","    for lat in range(-90, 90, 3):\n","        setLL(G0, 0, lat, 1)\n","        setLL(G0, 90, lat, 0.5)\n","        setLL(G0, 180, lat, 1)\n","        setLL(G0, 270, lat, 0.2)\n","    \n","    pl.figure(figsize=(12, 12))\n","    pl.axis('off')\n","    pl.imshow(G0.T, interpolation='none')\n","\n","if 0:\n","    #setLLV(G0, Gc, np.linspace(0, 359, 100), np.linspace(-90, 89, 100), 1)\n","    #setLLV(G, Gc, np.linspace(0, 359, 1000) / 180 * np.pi, np.linspace(66, 67, 1000) / 180 * np.pi, 1)\n","    setLLV(G, Gc, np.linspace(0, 359, 1000) / 180 * np.pi, np.linspace(66, 67, 1000) / 180 * np.pi, G.max())"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## Project the doppler/delay image into lon/lat\n","import astropy.coordinates as ac\n","import astropy.units as au\n","\n","def dopplerDelayToSphericalProjection(img_b, G, Gc,\n","                                      baud, pointing, \n","                                      srp_lon_deg,\n","                                      srp_lat_deg,\n","                                      freq_offset,\n","                                      delay_offset,\n","                                      freq_scale,\n","                                      doppler_angle):\n","    # Omit \"degraded data\" regions due to\n","    # - high N/S ambiguity near the edges (first few degrees of latitude), and\n","    # - grazing angle (last few degrees of latitude and longitude)\n","    #dlon = np.linspace(-85 / 180 * np.pi, 85 / 180 * np.pi, 8000)\n","    dlon = np.linspace(-90 / 180 * np.pi, 90 / 180 * np.pi, 8000)\n","    if pointing == 'N':\n","        #dlat = np.linspace(5 / 180 * np.pi, 85 / 180 * np.pi, 4000)  # Northern hemisphere\n","        dlat = np.linspace(-90 / 180 * np.pi, 90 / 180 * np.pi, 4000)  # Northern hemisphere\n","    else:\n","        dlat = np.linspace(-85 / 180 * np.pi, -5 / 180 * np.pi, 4000)  # Southern hemisphere\n","    dlon_mesh, dlat_mesh = np.meshgrid(dlon, dlat)\n","\n","    # Omit degraded data region due to SRP (within 7 deg of the center)\n","    mesh_valid = np.sqrt(dlat_mesh**2 + dlon_mesh**2) > (7 / 180 * np.pi)\n","    dlon_mesh = dlon_mesh[mesh_valid]\n","    dlat_mesh = dlat_mesh[mesh_valid]\n","\n","    c2 = img_b.shape[1] / 2\n","    cs = (c2 * freq_scale) * np.cos(dlat_mesh) * np.sin(dlon_mesh) + (c2 + freq_offset)\n","    _radius_km = 6051.8\n","    _row_dist_km = 299792.46 * baud * 1e-6 / 2 # km  Note: row is *round-trip-time* (double distance)\n","    rs = (1 - np.cos(dlon_mesh) * np.cos(dlat_mesh)) * (_radius_km / _row_dist_km) + delay_offset\n","\n","    rs = rs.astype('i')\n","    cs = cs.astype('i')\n","    valid = (cs >= 0) & (cs < img_b.shape[1]) & (rs >= 0) & (rs < img_b.shape[0])   \n","    \n","    if 0: # No SRP transform: as if the SRP were (0, 0)\n","        addLLV(G, Gc, dlon_mesh[valid], dlat_mesh[valid], img_b[rs[valid], cs[valid]])\n","\n","    if 1: # Transformed to the SRP and rotated by the \"doppler angle\"\n","        # Convert to unit spheroid cartesian coordinates.\n","        cdlat_mesh = np.cos(dlat_mesh)\n","        X = np.matrix((cdlat_mesh * np.cos(dlon_mesh),\n","                      cdlat_mesh * np.sin(dlon_mesh),\n","                      np.sin(dlat_mesh)))\n","\n","        # The S matrix rotates the coordinate system to center the SRP.\n","        clon = np.cos(srp_lon_deg / 180 * np.pi)\n","        clat = np.cos(srp_lat_deg / 180 * np.pi)\n","        slon = np.sin(srp_lon_deg / 180 * np.pi)\n","        slat = np.sin(srp_lat_deg / 180 * np.pi)\n","        S = np.matrix((( clon * clat,  slon * clat, slat),\n","                       (       -slon,         clon,    0),\n","                       (-clon * slat, -slon * slat, clat))) \n","\n","        ## The D matrix rotates the coordinates about the x axis by the apparent doppler angle:\n","        cnp = np.cos(doppler_angle.to(au.radian))\n","        snp = np.sin(doppler_angle.to(au.radian))\n","        D = np.matrix(((1,   0,    0),\n","                       (0, cnp, -snp),\n","                       (0, snp,  cnp))) \n","\n","        X = (S.T * D.T) * X\n","    \n","        # Convert back to lat/lon\n","        dlat_mesh = np.arcsin(X[2].A)[0]\n","        dlon_mesh = np.arctan2(X[1].A, X[0].A)[0]\n","        addLLV(G, Gc, dlon_mesh[valid], dlat_mesh[valid], img_b[rs[valid], cs[valid]])\n","\n","if _interactive:\n","    G = np.zeros((16000, 8000), dtype='f') # TODO: use a smaller data rep?\n","    Gc = np.zeros(G.shape, 'int')\n","    doppler_angle = apparentRotationAngle_poliastro(start_astrotime)\n","    dopplerDelayToSphericalProjection(img_b, G, Gc,\n","                                      lbl_dict['GEO_BAUD'], lbl_dict['GEO_POINTING'],\n","                                      hdata['ephemerides']['PDObsLon'][0],\n","                                      hdata['ephemerides']['PDObsLat'][0],\n","                                      freq_offset,\n","                                      delay_offset,\n","                                      freq_scale,\n","                                      doppler_angle)\n","    Gm = np.divide(G, Gc, where=Gc>0)\n","    os.makedirs(f\"{ROOT_PREFIX}/GLOBAL_TRIAGE\", exist_ok=True)\n","    pl.imsave(f\"{ROOT_PREFIX}/GLOBAL_TRIAGE/{filename[:-4]}_global.png\", Gm.T[::1, ::1], origin='lower')\n","\n","    if 1:\n","        pl.figure(figsize=(20, 10))\n","        pl.axis('off')\n","        pl.imshow(Gm.T[::4, ::4], cmap='gray', origin='lower')\n","        #pl.imshow(G.T[::4, ::4], cmap='gray', origin='lower')\n","        #pl.imshow(G.T, cmap='gray', origin='lower') # , interpolation='none')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Full processing pipeline (non-interactive)"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["# Full doppler/delay processing pipeline (with caching)\n","\n","FILTER_PREFIX = ROOT_PREFIX + \"FIT_GOOD/\"\n","FILTER = set()\n","filenames = os.listdir(FILTER_PREFIX)\n","for filename in filenames:\n","    FILTER.add(filename[:25])\n","\n","ROLLUP_CACHE = {}\n","filenames = os.listdir(ROOT_PREFIX + \"ROLLUP_GOOD/\")\n","for filename in filenames:\n","    if not filename.endswith('.png'): continue\n","    r = int(filename[33:].split('.')[0])\n","    ROLLUP_CACHE[filename[:25]] = r\n","\n","ROLL_CACHE = {}\n","filenames = os.listdir(ROOT_PREFIX + \"ROLL_GOOD/\")\n","for filename in filenames:\n","    if not filename.endswith('.png'): continue\n","    r = int(filename[31:].split('.')[0])\n","    ROLL_CACHE[filename[:25]] = r\n","\n","FIT_CACHE = {}\n","filenames = os.listdir(ROOT_PREFIX + \"FIT_GOOD/\")\n","for filename in filenames:\n","    if not filename.endswith('.png'): continue\n","    params = filename[30:-4].split(\"_\")\n","    FIT_CACHE[filename[:25]] = [int(params[0]), int(params[1]), float(params[2])]\n","\n","\n","def processDopplerDelayImage(filename, G=None, Gc=None, time_error=0, srp_lon_error=0, srp_lat_error=0, freq_offset_error=0, delay_offset_error=0, freq_scale_error=0, doppler_angle_error=0):\n","\n","    if not filename[:25] in FILTER: return\n","\n","    print('Processing', filename)\n","\n","    img = np.memmap(DATA_PREFIX + filename, dtype='<F', shape=(8191, 8192), mode='r')  # 'F' s complex single-precision: a complex number type of 2 32-bit precision floating-point numbers.\n","\n","    lbl_dict = parseLbl(filename)\n","\n","    # TODO: implement time error in ephemeris...\n","    hdata = loadOrFetchHorizonsData(filename)\n","\n","    # HACK: 1988 and 2020 data seem to have flipped doppler!?\n","    if lbl_dict['START_TIME'].startswith('1988') or lbl_dict['START_TIME'].startswith('2020'): img = np.fliplr(img)\n","\n","    # First do basic image processing: convert to magnitude from complex, and normalize.\n","    img_a = coarsePreprocessDopplerDelay(img)\n","\n","    # Second, coarse tune the range (rollup)\n","    #best_rollup = coarseTuneRollup(img_a)\n","    best_rollup = ROLLUP_CACHE[filename[:25]]  # Use cached rollup\n","    img_a = np.roll(img_a, best_rollup, axis=0)\n","\n","    # Third, fine process (which requires a tuned rollup)\n","    img_b = finePreprocessDopperDelay(img_a)\n","\n","    # Fourth, tune the symmetry (roll)\n","    #best_roll = coarseTuneRoll(img_b, lbl_dict, filename)\n","    best_roll = ROLL_CACHE[filename[:25]]  # Use cached roll\n","    img_b = np.roll(img_b, best_roll)\n","\n","    #best_fit_score, best_fit_parameters = fitDopplerDelayCurve(img_b, lbl_dict, filename)\n","    freq_offset, delay_offset, freq_scale = FIT_CACHE[filename[:25]]  # Use cached params\n","\n","    standalone_image = G is None or Gc is None\n","    if standalone_image: # Create new global map image and count image\n","        G = np.zeros((16000, 8000), dtype='f') # TODO: use a smaller data rep?\n","        Gc = np.zeros(G.shape, 'int')\n","\n","    start_astrotime = at.Time(lbl_dict['START_TIME'])\n","    doppler_angle = apparentRotationAngle_poliastro(start_astrotime)\n","    dopplerDelayToSphericalProjection(img_b, G, Gc,\n","                                      lbl_dict['GEO_BAUD'], lbl_dict['GEO_POINTING'],\n","                                      hdata['ephemerides']['PDObsLon'][0] + srp_lon_error,\n","                                      hdata['ephemerides']['PDObsLat'][0] + srp_lat_error,\n","                                      freq_offset + freq_offset_error,\n","                                      delay_offset + delay_offset_error,\n","                                      freq_scale + freq_scale_error,\n","                                      doppler_angle + doppler_angle_error * au.deg)\n","\n","    if standalone_image:\n","        Gm = np.divide(G, Gc, where=Gc>0)\n","        os.makedirs(f\"{ROOT_PREFIX}/GLOBAL_TRIAGE\", exist_ok=True)\n","\n","        if time_error or srp_lon_error or srp_lat_error or freq_offset_error or delay_offset_error or freq_scale_error or doppler_angle_error:\n","            # Full error details in filename\n","            global_filepath = f\"{ROOT_PREFIX}/GLOABAL_TRIAGE/{filename[:25]}_t_{time_error}_lon_{srp_lon_error}_lat_{srp_lat_error}_fo_{freq_offset_error}_do_{delay_offset_error}_fs_{freq_scale_error}_da_{doppler_angle_error}.png\"\n","        else:\n","            global_filepath = f\"{ROOT_PREFIX}/GLOBAL_TRIAGE/{filename[:25]}.png\"\n","\n","        pl.imsave(global_filepath, Gm.T, origin='lower')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Batch and commandline processing"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["#processDopplerDelayImage(\"venus_scp_20150811_174505.img\")\n","#processDopplerDelayImage(\"venus_ocp_20150816_171104.img\")\n","#processDopplerDelayImage(\"venus_ocp_19880617_151830.img\")\n","\n","if 1: # Test global combination\n","    file1 = \"venus_scp_20150815_170058.img\"\n","    #file1 = \"venus_ocp_19880617_150830.img\"\n","    file2 = \"venus_scp_20150812_155242.img\" \n","    #file3 = \"venus_scp_20170324_171055.img\"  # Appears to be ~0.2 degrees off?\n","    G = np.zeros((16000, 8000), dtype='f') # TODO: use a smaller data rep?\n","    Gc = np.zeros(G.shape, 'int')\n","    processDopplerDelayImage(file1, G, Gc)\n","    processDopplerDelayImage(file2, G, Gc)\n","    Gm = np.divide(G, Gc, where=Gc>0)\n","    pl.imsave(f\"{ROOT_PREFIX}/GLOBAL_TRIAGE/pair_{file1[:25]}_{file2[:25]}.png\", Gm.T[::1, ::1], origin='lower')"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["if 0: # Batch processing in notebook\n","    import os\n","    \n","    for filename in os.listdir(DATA_PREFIX):\n","        if not filename.endswith('.img'): continue\n","        #if not '_2012' in filename: continue\n","        if not '_2001' in filename: continue\n","        processDopplerDelayImage(filename)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## Main function to allow being run from the command-line.\n","import sys\n","\n","if __name__ == '__main__' and \"get_ipython\" not in dir():  # Not imported, not run from Jupyter/IPython\n","    args = sys.argv[1:]\n","    #print('args', args)\n","    processDopplerDelayImage(args[0])\n","    sys.exit()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["## Parallel self-running magic.\n","# TODO: reduce memory usage to allow more parallel instantiations!\n","cwd = os.getcwd()\n","\n","## 1. Convert to .py with (from \"My Drive/Colab Notebooks\" directory):\n","! jupyter nbconvert --to python Venera\\ -\\ Radar.ipynb\n","\n","## 2. Run the .py in parallel (from the venus data directory):\n","%cd $DATA_PREFIX\n","! ls -1 *2020*.img | xargs -n 1 -P 4 python3 ${cwd}/Venera\\ -\\ Radar.py\n","#! ls -1 *.img | xargs -n 1 -P 4 python3 ${cwd}/Venera\\ -\\ Radar.py"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Hunting down registration errors\n","\n","There appears to be a significant amount of residual error when projecting the doppler/range images into global coordinates and then trying to stack them.\n","\n","Campbell et al didn't try to chase down these errors (personal communication), and instead fit the images to known anchor points. (did they set up an under-constrained problem?). One interesting data point is that Campbell's fitted doppler angles are all around 10 degrees, whereas my doppler angles derived from ephemeris data is often as much as 20 degrees. Given the error characteristics of the ephemeris data (see discussion below), it does not seem plausible for there to be 10 degrees of error!\n","\n","This seems unsatisfactory to me: the physics of this experiment are pretty clean, and so the projections should be \n","\n","There are two ways to hunt for the sources of error -- and it is likely that there are multiple. The first is to double-check the processing pipeline for mistakes. The second is to characterize the *expected* error due to each source and then compare these expectations with the actual observed error to see if it is fully explained.\n","\n","# Known error sources\n","1. My fit parameters. Specifically, sub-radar point range (called rollup and delay_offset in the code), range rate (called roll and freq_offset in the code), and doppler spreading (called freq_scale in the code). Due to unknown offsets used during the original data collection process, delay_offset and freq_offset parameters can't be absolutely determined, and must be calculated by fitting. On the other hand, they *should* be robust to fitting. Freq_scale on the other hand should be directly correlated with the \"lateral\" relative observer velocity.\n","\n","[DONE: calculate expected freq_scale and compare with the fit results.  -->  See analysis above.]\n","\n","[TODO: calculate expected projection error sensitivity due to errors in delay_offset and freq_offset)]\n","\n","2. The ephemeris projections. Both JPL Horizons and poliastro use the DE44X family. From https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/planets/de440_and_de441.pdf, orbital position of Venus is estimated to have an rms residual of about 8 m. From https://escholarship.org/content/qt2dn1012s/qt2dn1012s.pdf?t=pv3anr, errors in the DE44X rotation rate for Venus amount to approximately 0.2 degrees over 30 years.\n","\n","[NOTE: I don't yet have an estimate of error for the axis of Venus]\n","\n","In general,\n","- error in the planetary position would result in projected errors in the SRP and thus joint lat/lon errors... but perhaps mostly in longitude.\n","- error in the rotation rate would result in projected errors in longitude.\n","- error in the polar axis would result in errors in latitude and doppler angle.\n","\n","# Potential error sources\n","1. My transformation code\n","2. My doppler angle calculation code\n","3. Some type of time error (wrong time zone? JD to TBD?)\n","4. Some missing systematic correction factor, that is not included in the standard descriptions of the doppler/delay image processing pipeline.\n","5. Some missing noise factor that can cause minute-to-minute, day-to-day, or year-to-year error. Examples could be perturbations due to atmospheric or magnetospheric conditions, issues with the radio telescope.\n","\n","[TODO: one way to chase down possible noise factors is to look at the noise characteristics of the projected images, and see what sort of time constants the noise appears to have. If the noise is small minute-to-minute but large year-to-year, that helps us constrain the likely sources.]"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["if 0: # debug: Compare \"orthogonal\" velocity to freq_scale\n","    FILENAME = []\n","    FREQ_OFFSET = []\n","    DELAY_OFFSET = []\n","    FREQ_SCALE = []\n","    ORTHO_VEL = []\n","    GEO_BAUD = []\n","    filenames = os.listdir(ROOT_PREFIX + \"FIT_TRIAGE_2/\")\n","    for filename in filenames:\n","        if not filename.endswith('.png'): continue\n","        params = filename[30:-4].split(\"_\")\n","        lbl_dict = parseLbl(filename[:25])\n","        SRP = pickle.load(open(DATA_PREFIX + filename[:25] + '_horizons.pkl', 'rb'))\n","        start_astrotime = SRP['start_astrotime']\n","\n","        FILENAME.append(filename[:25])\n","        FREQ_OFFSET.append(int(params[0]))\n","        DELAY_OFFSET.append(int(params[1]))\n","        FREQ_SCALE.append(float(params[2]))\n","        GEO_BAUD.append(lbl_dict['GEO_BAUD'])\n","        ORTHO_VEL.append(orthogonalVelocity_poliastro(start_astrotime))\n","\n","    FREQ_OFFSET = np.array(FREQ_OFFSET)\n","    DELAY_OFFSET = np.array(DELAY_OFFSET)\n","    FREQ_SCALE = np.array(FREQ_SCALE)\n","    ORTHO_VEL = np.array(ORTHO_VEL)\n","    GEO_BAUD = np.array(GEO_BAUD)\n","\n","if 0:\n","    #plt.figure(figsize=(12,12))\n","    #plt.plot(FREQ_SCALE)\n","    #plt.plot(ORTHO_VEL)\n","    for y in (\"1988\", \"2001\", \"2012\", \"2015\", \"2017\", \"2020\"):\n","    #for y in [f\"201508{i:02d}\" for i in range(10, 17)]:\n","    #for y in [f\"201703{i:02d}\" for i in range(21, 28)]:\n","        m = [f\"_{y}\" in x for x in FILENAME]\n","        #plt.plot(FREQ_OFFSET[m], '.', label=y)\n","        #plt.plot(DELAY_OFFSET[m], '.', label=y)\n","        #plt.plot(ORTHO_VEL[m], '.', label=y)\n","        plt.plot(1 / FREQ_SCALE[m] * GEO_BAUD[m], ORTHO_VEL[m], '.', label=y)\n","    plt.legend()\n","    plt.title('Baud-corrected fit scale factor vs \"orthogonal\" velocity')\n","#    plt.title('\"orthogonal\" velocity')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["While the delay-offset and frequency-offset are likely not recoverable due to how the data files were created, the frequency-scale *should* be something we can calculate directly. The frequency-scale factor is directly related to the \"orthogonal\" velocity -- the component of Venus-to-observer motion that is perpendicular to the line between the two. The higher this orthogonal velocity, the higher the apparent velocity of the limbs, symmetrically.\n","\n","We do need to correct the bandwidth for different years using their baud rate.\n","\n","| Year | Baud rates (microseconds) |\n","| ---- | ------------------------- |\n","| 1988 | 4.0 |\n","| 2001 | 4.2 |\n","| 2012 | 3.8 |\n","| 2015 | 3.8 |\n","| 2017 | 3.9 |\n","| 2020 | 3.8 |\n","\n","As a result, fit-frequency-scale / baud should be proportional to orthogonal velocity.\n","\n","All of the years line up nicely *except* 2015 and 2017, which seem to have a time-varying bias. 2015 in particular has the same baud as 2012 and 2020.\n","Was there some approximate correction frequency-scale factor applied during the 2015 and 2017 observation seasons?\n","\n","Another possiblity is that there is some sort of systematic fit error that is affecting only the 2015 and 2017 looks.\n","\n","From \"Arecibo Radar Maps of Venus from 1988 to 2020\": \n","\"Two methods were used to compensate for the gross\n","Doppler shift between the observing station and Venus. For the\n","1988 data, no compensation was applied during the observations,\n","so an optimum change in Doppler frequency with time was\n","derived using an autofocus technique during the correlation with\n","the PN code. For all later data, ephemerides provided by different\n","programs were used to impose a time-varying frequency shift to\n","place the subradar location on Venus at zero Doppler.\"\n","\n","Taken all together, this does increase our confidence in the quality of the frequency-scale factor fit.\n","* The strong linear fit for all years *other* than 2015 and 2017 is directly encouraging\n","* The fact that 2015 and 2017 appear to follow some sort of curve gives confidence that the fit has low noise, though it does raise the question of the source of the curve."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python"}},"outputs":[],"source":["if 0: ## debug: Error experiment...\n","\n","    filename = \"venus_scp_19880617_160050.img\"\n","    \n","    # No noise\n","    processDopplerDelayImage(filename)\n","    \n","    for t in (-10000, -1000, -100, -10, 10, 100, 1000, 10000):\n","        processDopplerDelayImage(filename, time_error=t)\n","    \n","    for lon in (-10, -5, -2, -1, 1, 2, 5, 10): \n","        processDopplerDelayImage(filename, srp_lon_error=lon)\n","    \n","    for lat in (-10, -5, -2, -1, 1, 2, 5, 10): \n","        processDopplerDelayImage(filename, srp_lat_error=lat)\n","    \n","    for fo in (-10, -5, -2, -1, 1, 2, 5, 10):\n","        processDopplerDelayImage(filename, freq_offset_error=fo)\n","    \n","    for do in (-10, -5, -2, -1, 1, 2, 5, 10):\n","        processDopplerDelayImage(filename, delay_offset_error=do)\n","    \n","    for fs in (-0.5, -0.2, -0.1, -0.01, 0.01, 0.1, 0.2, 0.5):\n","        processDopplerDelayImage(filename, freq_scale_error=fs)\n","    \n","    for da in (-10, -5, -2, -1, 1, 2, 5, 10):\n","        processDopplerDelayImage(filename, doppler_angle_error=da)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Error analysis\n","\n","Size of error needed to explain observed magnitude of misalignment (a combination of errors might be necessary!)\n","- Time: ~50000 s\n","- Lon: 10 degrees\n","- Lat: 5 degrees\n","- Frequency offset: >> 10 pixels\n","- Delay offset: >> 10 pixels\n","- Frequency scale factor: +-20%\n","- Doppler angle: 5 degrees\n","\n","Expected error sizes:\n","- Time: ~300 s (due to round-trip time)\n","- Lon: ??? (from ephemeris)\n","- Lat: ??? (from ephemeris)\n","- Frequency offset: 2 pixels\n","- Delay offset: 2 pixels\n","- Frequency scale factor: +-1%\n","- Doppler angle: ??? degrees (from ephemeris)\n","\n","\n","NOTE: the lat/lon error sizes are on the scale of the changes in the sub-radar point -- have I just messed up the transform?!?"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPzhu/Sb46Ue9ZfoPLkzO03","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"vscode":{"interpreter":{"hash":"40fefee005f43b138c30243dc2ffca6e84360bada605f47b1fde319709267cca"}}},"nbformat":4,"nbformat_minor":0}
